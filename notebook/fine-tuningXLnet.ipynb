{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7870213d-452e-4761-bf41-353d3e95c981",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: http://mirrors.aliyun.com/pypi/simple\n",
      "Collecting transformers\n",
      "  Downloading http://mirrors.aliyun.com/pypi/packages/52/82/b62f139e77c70fab2763d99d83a77a27a179502430851144a1765e37f5ad/transformers-4.19.2-py3-none-any.whl (4.2 MB)\n",
      "\u001b[K     |████████████████████████████████| 4.2 MB 642 kB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.17 in ./miniconda3/lib/python3.8/site-packages (from transformers) (1.21.4)\n",
      "Collecting regex!=2019.12.17\n",
      "  Downloading http://mirrors.aliyun.com/pypi/packages/d8/e8/4331ed657cbea0fcc16bc57320d3a34e734d6a49dff1760beeedf827d26d/regex-2022.4.24-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (764 kB)\n",
      "\u001b[K     |████████████████████████████████| 764 kB 575 kB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: requests in ./miniconda3/lib/python3.8/site-packages (from transformers) (2.25.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in ./miniconda3/lib/python3.8/site-packages (from transformers) (4.61.2)\n",
      "Collecting pyyaml>=5.1\n",
      "  Downloading http://mirrors.aliyun.com/pypi/packages/d7/42/7ad4b6d67a16229496d4f6e74201bdbebcf4bc1e87d5a70c9297d4961bd2/PyYAML-6.0-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (701 kB)\n",
      "\u001b[K     |████████████████████████████████| 701 kB 629 kB/s eta 0:00:01\n",
      "\u001b[?25hCollecting filelock\n",
      "  Downloading http://mirrors.aliyun.com/pypi/packages/e8/74/48523f5206b0930f7c6b312890c7ab285dba55cea3f0a303999c5425df08/filelock-3.7.0-py3-none-any.whl (10 kB)\n",
      "Requirement already satisfied: packaging>=20.0 in ./miniconda3/lib/python3.8/site-packages (from transformers) (21.3)\n",
      "Collecting tokenizers!=0.11.3,<0.13,>=0.11.1\n",
      "  Downloading http://mirrors.aliyun.com/pypi/packages/36/fa/e22ebbcaeecd9bd04efa30f7ec43ccf1501c97615c9af3bbf13a77ce0b81/tokenizers-0.12.1-cp38-cp38-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.6 MB)\n",
      "\u001b[K     |████████████████████████████████| 6.6 MB 644 kB/s eta 0:00:01\n",
      "\u001b[?25hCollecting huggingface-hub<1.0,>=0.1.0\n",
      "  Downloading http://mirrors.aliyun.com/pypi/packages/c1/f2/d6542f1e29b803442e058f7a1b52313bea37da46517b1e840ff2f166450c/huggingface_hub-0.6.0-py3-none-any.whl (84 kB)\n",
      "\u001b[K     |████████████████████████████████| 84 kB 640 kB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: typing-extensions>=3.7.4.3 in ./miniconda3/lib/python3.8/site-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (4.0.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in ./miniconda3/lib/python3.8/site-packages (from packaging>=20.0->transformers) (3.0.6)\n",
      "Requirement already satisfied: idna<3,>=2.5 in ./miniconda3/lib/python3.8/site-packages (from requests->transformers) (2.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./miniconda3/lib/python3.8/site-packages (from requests->transformers) (2021.5.30)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in ./miniconda3/lib/python3.8/site-packages (from requests->transformers) (4.0.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in ./miniconda3/lib/python3.8/site-packages (from requests->transformers) (1.26.6)\n",
      "Installing collected packages: pyyaml, filelock, tokenizers, regex, huggingface-hub, transformers\n",
      "Successfully installed filelock-3.7.0 huggingface-hub-0.6.0 pyyaml-6.0 regex-2022.4.24 tokenizers-0.12.1 transformers-4.19.2\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "Looking in indexes: http://mirrors.aliyun.com/pypi/simple\n",
      "Collecting tensorflow\n",
      "  Downloading http://mirrors.aliyun.com/pypi/packages/a0/27/1044b3ea6554b49c1219e5bd1a133880a1266ccc3980602be54786c8f277/tensorflow-2.9.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (511.7 MB)\n",
      "\u001b[K     |████████████████████████████████| 511.7 MB 579 kB/s eta 0:00:01     |███████████████████████████▏    | 434.7 MB 571 kB/s eta 0:02:15     |███████████████████████████████ | 496.1 MB 567 kB/s eta 0:00:28\n",
      "\u001b[?25hRequirement already satisfied: setuptools in ./miniconda3/lib/python3.8/site-packages (from tensorflow) (52.0.0.post20210125)\n",
      "Requirement already satisfied: protobuf>=3.9.2 in ./miniconda3/lib/python3.8/site-packages (from tensorflow) (3.19.1)\n",
      "Collecting google-pasta>=0.1.1\n",
      "  Downloading http://mirrors.aliyun.com/pypi/packages/a3/de/c648ef6835192e6e2cc03f40b19eeda4382c49b5bafb43d88b931c4c74ac/google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "\u001b[K     |████████████████████████████████| 57 kB 462 kB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: packaging in ./miniconda3/lib/python3.8/site-packages (from tensorflow) (21.3)\n",
      "Requirement already satisfied: six>=1.12.0 in ./miniconda3/lib/python3.8/site-packages (from tensorflow) (1.16.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in ./miniconda3/lib/python3.8/site-packages (from tensorflow) (1.42.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in ./miniconda3/lib/python3.8/site-packages (from tensorflow) (1.0.0)\n",
      "Collecting gast<=0.4.0,>=0.2.1\n",
      "  Downloading http://mirrors.aliyun.com/pypi/packages/b6/48/583c032b79ae5b3daa02225a675aeb673e58d2cb698e78510feceb11958c/gast-0.4.0-py3-none-any.whl (9.8 kB)\n",
      "Collecting keras<2.10.0,>=2.9.0rc0\n",
      "  Downloading http://mirrors.aliyun.com/pypi/packages/ff/ff/f25909606aed26981a8bd6d263f89d64a20ca5e5316e6aafb4c75d9ec8ae/keras-2.9.0-py2.py3-none-any.whl (1.6 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.6 MB 580 kB/s eta 0:00:01\n",
      "\u001b[?25hCollecting h5py>=2.9.0\n",
      "  Downloading http://mirrors.aliyun.com/pypi/packages/d4/d8/6a121064f0e357dc24e5ab53945e9ec057bb1e5ca7da60355d06c89a3d36/h5py-3.6.0-cp38-cp38-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (4.5 MB)\n",
      "\u001b[K     |████████████████████████████████| 4.5 MB 605 kB/s eta 0:00:01     |██████████████████▉             | 2.6 MB 597 kB/s eta 0:00:04\n",
      "\u001b[?25hCollecting tensorboard<2.10,>=2.9\n",
      "  Downloading http://mirrors.aliyun.com/pypi/packages/69/80/a3abccc4ea941c36741751206e40e619afe28652cf76f74cfa4c3e4248ba/tensorboard-2.9.0-py3-none-any.whl (5.8 MB)\n",
      "\u001b[K     |████████████████████████████████| 5.8 MB 587 kB/s eta 0:00:01\n",
      "\u001b[?25hCollecting termcolor>=1.1.0\n",
      "  Downloading http://mirrors.aliyun.com/pypi/packages/8a/48/a76be51647d0eb9f10e2a4511bf3ffb8cc1e6b14e9e4fab46173aa79f981/termcolor-1.1.0.tar.gz (3.9 kB)\n",
      "Collecting tensorflow-estimator<2.10.0,>=2.9.0rc0\n",
      "  Downloading http://mirrors.aliyun.com/pypi/packages/61/e1/a72ec68403d91ba433018db58859fd4706642aa9d0fb44ff778934fc4c2c/tensorflow_estimator-2.9.0-py2.py3-none-any.whl (438 kB)\n",
      "\u001b[K     |████████████████████████████████| 438 kB 544 kB/s eta 0:00:01\n",
      "\u001b[?25hCollecting libclang>=13.0.0\n",
      "  Downloading http://mirrors.aliyun.com/pypi/packages/ab/2f/c6f380aec0b064bccbd81141fecba9862b5634c838f13fff727adc84ceb9/libclang-14.0.1-py2.py3-none-manylinux1_x86_64.whl (14.5 MB)\n",
      "\u001b[K     |████████████████████████████████| 14.5 MB 248 kB/s eta 0:00:01\n",
      "\u001b[?25hCollecting opt-einsum>=2.3.2\n",
      "  Downloading http://mirrors.aliyun.com/pypi/packages/bc/19/404708a7e54ad2798907210462fd950c3442ea51acc8790f3da48d2bee8b/opt_einsum-3.3.0-py3-none-any.whl (65 kB)\n",
      "\u001b[K     |████████████████████████████████| 65 kB 631 kB/s eta 0:00:01\n",
      "\u001b[?25hCollecting wrapt>=1.11.0\n",
      "  Downloading http://mirrors.aliyun.com/pypi/packages/36/ee/944dc7e5462662270e8a379755bcc543fc8f09029866288060dc163ed5b4/wrapt-1.14.1-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (81 kB)\n",
      "\u001b[K     |████████████████████████████████| 81 kB 606 kB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: typing-extensions>=3.6.6 in ./miniconda3/lib/python3.8/site-packages (from tensorflow) (4.0.0)\n",
      "Collecting keras-preprocessing>=1.1.1\n",
      "  Downloading http://mirrors.aliyun.com/pypi/packages/79/4c/7c3275a01e12ef9368a892926ab932b33bb13d55794881e3573482b378a7/Keras_Preprocessing-1.1.2-py2.py3-none-any.whl (42 kB)\n",
      "\u001b[K     |████████████████████████████████| 42 kB 662 kB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.20 in ./miniconda3/lib/python3.8/site-packages (from tensorflow) (1.21.4)\n",
      "Collecting astunparse>=1.6.0\n",
      "  Downloading http://mirrors.aliyun.com/pypi/packages/2b/03/13dde6512ad7b4557eb792fbcf0c653af6076b81e5941d36ec61f7ce6028/astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Collecting flatbuffers<2,>=1.12\n",
      "  Downloading http://mirrors.aliyun.com/pypi/packages/eb/26/712e578c5f14e26ae3314c39a1bdc4eb2ec2f4ddc89b708cf8e0a0d20423/flatbuffers-1.12-py2.py3-none-any.whl (15 kB)\n",
      "Collecting tensorflow-io-gcs-filesystem>=0.23.1\n",
      "  Downloading http://mirrors.aliyun.com/pypi/packages/ba/7c/13c8a2e1cf7e2f4a898c75ff388a1873aefb8e3037368030545576bd67ee/tensorflow_io_gcs_filesystem-0.25.0-cp38-cp38-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (2.1 MB)\n",
      "\u001b[K     |████████████████████████████████| 2.1 MB 536 kB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: wheel<1.0,>=0.23.0 in ./miniconda3/lib/python3.8/site-packages (from astunparse>=1.6.0->tensorflow) (0.36.2)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in ./miniconda3/lib/python3.8/site-packages (from tensorboard<2.10,>=2.9->tensorflow) (0.4.6)\n",
      "Requirement already satisfied: markdown>=2.6.8 in ./miniconda3/lib/python3.8/site-packages (from tensorboard<2.10,>=2.9->tensorflow) (3.3.6)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in ./miniconda3/lib/python3.8/site-packages (from tensorboard<2.10,>=2.9->tensorflow) (2.0.2)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in ./miniconda3/lib/python3.8/site-packages (from tensorboard<2.10,>=2.9->tensorflow) (0.6.1)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in ./miniconda3/lib/python3.8/site-packages (from tensorboard<2.10,>=2.9->tensorflow) (2.25.1)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in ./miniconda3/lib/python3.8/site-packages (from tensorboard<2.10,>=2.9->tensorflow) (2.3.3)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in ./miniconda3/lib/python3.8/site-packages (from tensorboard<2.10,>=2.9->tensorflow) (1.8.0)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in ./miniconda3/lib/python3.8/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow) (4.2.4)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in ./miniconda3/lib/python3.8/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow) (0.2.8)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in ./miniconda3/lib/python3.8/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow) (4.8)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in ./miniconda3/lib/python3.8/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.10,>=2.9->tensorflow) (1.3.0)\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in ./miniconda3/lib/python3.8/site-packages (from markdown>=2.6.8->tensorboard<2.10,>=2.9->tensorflow) (4.8.2)\n",
      "Requirement already satisfied: zipp>=0.5 in ./miniconda3/lib/python3.8/site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.10,>=2.9->tensorflow) (3.6.0)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in ./miniconda3/lib/python3.8/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow) (0.4.8)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in ./miniconda3/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow) (1.26.6)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in ./miniconda3/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow) (4.0.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./miniconda3/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow) (2021.5.30)\n",
      "Requirement already satisfied: idna<3,>=2.5 in ./miniconda3/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow) (2.10)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in ./miniconda3/lib/python3.8/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.10,>=2.9->tensorflow) (3.1.1)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in ./miniconda3/lib/python3.8/site-packages (from packaging->tensorflow) (3.0.6)\n",
      "Building wheels for collected packages: termcolor\n",
      "  Building wheel for termcolor (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for termcolor: filename=termcolor-1.1.0-py3-none-any.whl size=4829 sha256=73ba99b82f6960098dc53b95ea8448180467c873207b4c9a6aa7e63948d0a308\n",
      "  Stored in directory: /root/.cache/pip/wheels/77/83/72/2f8fd2274618dabac14ac44c425333b4d9cd6cb296243af7f1\n",
      "Successfully built termcolor\n",
      "Installing collected packages: wrapt, termcolor, tensorflow-io-gcs-filesystem, tensorflow-estimator, tensorboard, opt-einsum, libclang, keras-preprocessing, keras, h5py, google-pasta, gast, flatbuffers, astunparse, tensorflow\n",
      "  Attempting uninstall: tensorboard\n",
      "    Found existing installation: tensorboard 2.7.0\n",
      "    Uninstalling tensorboard-2.7.0:\n",
      "      Successfully uninstalled tensorboard-2.7.0\n",
      "Successfully installed astunparse-1.6.3 flatbuffers-1.12 gast-0.4.0 google-pasta-0.2.0 h5py-3.6.0 keras-2.9.0 keras-preprocessing-1.1.2 libclang-14.0.1 opt-einsum-3.3.0 tensorboard-2.9.0 tensorflow-2.9.0 tensorflow-estimator-2.9.0 tensorflow-io-gcs-filesystem-0.25.0 termcolor-1.1.0 wrapt-1.14.1\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers\n",
    "!pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0efd1339-aca4-4eef-839c-3f8c7dbb7049",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'|===========================================================================|\\n|                  PyTorch CUDA memory summary, device ID 0                 |\\n|---------------------------------------------------------------------------|\\n|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |\\n|===========================================================================|\\n|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\\n|---------------------------------------------------------------------------|\\n| Allocated memory      |       0 B  |       0 B  |       0 B  |       0 B  |\\n|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |\\n|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |\\n|---------------------------------------------------------------------------|\\n| Active memory         |       0 B  |       0 B  |       0 B  |       0 B  |\\n|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |\\n|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |\\n|---------------------------------------------------------------------------|\\n| GPU reserved memory   |       0 B  |       0 B  |       0 B  |       0 B  |\\n|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |\\n|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |\\n|---------------------------------------------------------------------------|\\n| Non-releasable memory |       0 B  |       0 B  |       0 B  |       0 B  |\\n|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |\\n|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |\\n|---------------------------------------------------------------------------|\\n| Allocations           |       0    |       0    |       0    |       0    |\\n|       from large pool |       0    |       0    |       0    |       0    |\\n|       from small pool |       0    |       0    |       0    |       0    |\\n|---------------------------------------------------------------------------|\\n| Active allocs         |       0    |       0    |       0    |       0    |\\n|       from large pool |       0    |       0    |       0    |       0    |\\n|       from small pool |       0    |       0    |       0    |       0    |\\n|---------------------------------------------------------------------------|\\n| GPU reserved segments |       0    |       0    |       0    |       0    |\\n|       from large pool |       0    |       0    |       0    |       0    |\\n|       from small pool |       0    |       0    |       0    |       0    |\\n|---------------------------------------------------------------------------|\\n| Non-releasable allocs |       0    |       0    |       0    |       0    |\\n|       from large pool |       0    |       0    |       0    |       0    |\\n|       from small pool |       0    |       0    |       0    |       0    |\\n|---------------------------------------------------------------------------|\\n| Oversize allocations  |       0    |       0    |       0    |       0    |\\n|---------------------------------------------------------------------------|\\n| Oversize GPU segments |       0    |       0    |       0    |       0    |\\n|===========================================================================|\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.cuda.empty_cache()\n",
    "torch.cuda.memory_summary(device=None, abbreviated=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "309ef23a-18bb-43f4-84dc-a920963d524b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-22 15:29:07.760183: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import csv\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import TensorDataset\n",
    "from PIL import Image, ImageFile\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision import models\n",
    "import torch.utils.model_zoo as model_zoo\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import os\n",
    "import json\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from transformers import XLNetModel, XLNetTokenizerFast\n",
    "import operator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5df42a46-aeb2-4992-847d-cea7b6c96f59",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at xlnet-base-cased were not used when initializing XLNetModel: ['lm_loss.bias', 'lm_loss.weight']\n",
      "- This IS expected if you are initializing XLNetModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLNetModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda')\n",
    "tokenizer = XLNetTokenizerFast.from_pretrained(\"xlnet-base-cased\")\n",
    "pre_trained_model = XLNetModel.from_pretrained(\"xlnet-base-cased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e6b78e08-8424-40fc-8338-de7618b480fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContextQuType:\n",
    "    def __init__(self, tokenizer, length, ids, context, question):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.codeLength = length\n",
    "        self.ids = ids\n",
    "        self.context = context\n",
    "        self.question = question\n",
    "        self.after_encode = 0\n",
    "        \n",
    "    def convert(self):\n",
    "        if self.after_encode==0:\n",
    "            after_encode = tokenizer(self.question,self.context,max_length=self.codeLength,padding=\"max_length\")\n",
    "        return torch.stack([torch.tensor(after_encode['input_ids'][-512:]),torch.tensor(after_encode['token_type_ids'][-512:]),torch.tensor(after_encode['attention_mask'][-512:])])\n",
    "        \n",
    "class LabelType:\n",
    "    def __init__(self, tokenizer, length, CQu, has_ans, text, start):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.codeLength = length\n",
    "        self.has_ans = has_ans\n",
    "        self.text = text\n",
    "        self.start = start\n",
    "        self.CQu = CQu\n",
    "    \n",
    "    def convert(self):\n",
    "        # make groundtruth\n",
    "        S = [0.0] * self.codeLength\n",
    "        E = [0.0] * self.codeLength\n",
    "        background = tokenizer.encode(self.CQu.question,self.CQu.context,max_length=self.codeLength,padding=\"max_length\")\n",
    "        length_exceed = False\n",
    "        if len(background) > self.codeLength:\n",
    "            length_exceed = True\n",
    "        if self.has_ans:\n",
    "            after_encode = tokenizer.encode(self.text)[:-2]\n",
    "            ans_length = len(after_encode)\n",
    "            after_answer = tokenizer.encode(self.CQu.context[self.start:])\n",
    "            if length_exceed:\n",
    "                start_id = len(background) - len(after_answer)\n",
    "                end_id = start_id + ans_length -1\n",
    "                if tokenizer.decode(background[start_id:end_id+1])!=self.text:\n",
    "                    lower = max(start_id-3,0)\n",
    "                    upper = min(end_id+4,len(background))\n",
    "                    ans_found = False\n",
    "                    for i in range(lower,upper):\n",
    "                        for j in range(i+1,upper):\n",
    "                            if tokenizer.decode(background[i:j])==self.text:\n",
    "                                start_id = i\n",
    "                                end_id = j-1\n",
    "                                ans_found = True\n",
    "                                break\n",
    "                            if ans_found:\n",
    "                                break\n",
    "                if end_id >= self.codeLength:\n",
    "                    end_id = self.codeLength-1\n",
    "                if start_id >= self.codeLength:\n",
    "                    start_id = 0\n",
    "                    end_id = 0\n",
    "                S[start_id] = 1.0\n",
    "                E[end_id] = 1.0\n",
    "            else:\n",
    "                start_id = len(background) - len(after_answer)\n",
    "                end_id = start_id + ans_length -1\n",
    "                if tokenizer.decode(background[start_id:end_id+1])!=self.text:\n",
    "                    lower = max(start_id-3,0)\n",
    "                    upper = min(end_id+4,len(background))\n",
    "                    ans_found = False\n",
    "                    for i in range(lower,upper):\n",
    "                        for j in range(i+1,upper):\n",
    "                            candidate = tokenizer.decode(background[i:j])\n",
    "                            if len(candidate)>0:\n",
    "                                if candidate[0]=='$' and self.text[0]!='$':\n",
    "                                    candidate=candidate[1:]\n",
    "                                if candidate[0]!='$' and self.text[0]=='$':\n",
    "                                    candidate='$'+candidate\n",
    "                            if candidate==self.text:\n",
    "                                start_id = i\n",
    "                                end_id = j-1\n",
    "                                ans_found = True\n",
    "                                break\n",
    "                            if ans_found:\n",
    "                                break\n",
    "                S[start_id] = 1.0\n",
    "                E[end_id] = 1.0\n",
    "            #if tokenizer.decode(background[start_id:end_id+1])!=self.text:\n",
    "                #print(tokenizer.convert_ids_to_tokens(background))\n",
    "                #print(tokenizer.decode(after_answer[:ans_length])+'|||'+tokenizer.decode(background[start_id:end_id+1])+'|||'+self.text)\n",
    "                #print(len(background),len(after_answer),start_id,end_id)\n",
    "                #print(self.CQu.context[self.start:self.start+len(self.text)]+'|||'+tokenizer.decode(after_encode)+'|||',tokenizer.convert_ids_to_tokens(after_encode))\n",
    "        else:\n",
    "            S[-1] = 1.0\n",
    "            E[-1] = 1.0\n",
    "        return torch.stack([torch.tensor(S),torch.tensor(E)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5b96639f-9c3d-4c7b-9b74-fde3927766f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(load_path,tokenizer,length):\n",
    "    with open(load_path) as json_data:\n",
    "        dev = json.load(json_data)['data']\n",
    "    CQu = []\n",
    "    Label = []\n",
    "    for data in dev:\n",
    "        for paragraphs in data['paragraphs']:\n",
    "            context = paragraphs['context']\n",
    "            for qas in paragraphs['qas']:\n",
    "                question = qas['question']\n",
    "                ids = qas['id']\n",
    "                ctx = ContextQuType(tokenizer,length,ids,context,question)\n",
    "                CQu.append(ctx)\n",
    "                if qas['is_impossible']:\n",
    "                    Label.append(LabelType(tokenizer,length,ctx,False,'',0))\n",
    "                else:\n",
    "                    answer = qas['answers'][0]\n",
    "                    Label.append(LabelType(tokenizer,length,ctx,True,answer['text'],answer['answer_start']))\n",
    "    return CQu,Label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7dc85e75-9fa2-4082-bd42-fef5da59b14c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OutputPair:\n",
    "    def __init__(self,logits_start,logits_end):\n",
    "        self.logits_start = logits_start\n",
    "        self.logits_end = logits_end\n",
    "\n",
    "class CrossEntropy(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CrossEntropy, self).__init__()\n",
    "    def forward(self,outputs,labels):\n",
    "        log_soft = F.log_softmax(outputs.logits_start,dim=1)\n",
    "        loss_start = F.cross_entropy(log_soft, labels[:,0])\n",
    "        loss_end   = F.cross_entropy(F.log_softmax(outputs.logits_end,dim=1), labels[:,1])\n",
    "        return loss_start + loss_end / 2.0\n",
    "\n",
    "class BERTQuA(nn.Module):\n",
    "    def __init__(self, bert, tokenizer,code_length,device,epsilon = 1.0):\n",
    "        super(BERTQuA, self).__init__()\n",
    "        self.tokenizer = tokenizer\n",
    "        self.device = device\n",
    "        self.encoder = bert.to(device)\n",
    "        self.code_length = code_length\n",
    "        self.output_start = nn.Linear(768,1).to(device)\n",
    "        self.output_end = nn.Linear(768,1).to(device)\n",
    "        self.epsilon = 1.0\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        tokens_X, segments_X, masks = inputs[:,0],inputs[:,1],inputs[:,2]\n",
    "        encoded_X = self.encoder(tokens_X, token_type_ids=segments_X, attention_mask=masks).last_hidden_state\n",
    "        return OutputPair(self.output_start(encoded_X),self.output_end(encoded_X))\n",
    "        \n",
    "    def prediction(self,question,answer_text):\n",
    "        ctx = ContextQuType(self.tokenizer,self.code_length,'',answer_text,question)\n",
    "        inputs = torch.stack([ctx.convert()]).to(device)\n",
    "        token_ids = tokenizer(question,answer_text,max_length=self.code_length,padding=\"max_length\")['input_ids']\n",
    "        outputs = self.forward(inputs)\n",
    "        start_scores = outputs.logits_start.reshape(-1)\n",
    "        end_scores = outputs.logits_end.reshape(-1)\n",
    "        no_ans_scores = start_scores[-1] + end_scores[-1]\n",
    "        answer = ''\n",
    "        tokens = tokenizer.convert_ids_to_tokens(token_ids)\n",
    "        #print(outputs.logits_start.reshape(-1))\n",
    "        #print(outputs.logits_end.reshape(-1))\n",
    "        if (torch.max(start_scores) + torch.max(end_scores) <= no_ans_scores +  self.epsilon):\n",
    "            answer = ''\n",
    "        else:\n",
    "            add_space = False\n",
    "            answer_start = torch.argmax(start_scores)\n",
    "            answer_end = torch.argmax(end_scores)\n",
    "            answer = self.tokenizer.decode(token_ids[answer_start:answer_end+1])\n",
    "        return answer\n",
    "    def predictionLabel(self,label):\n",
    "        #ctx = ContextQuType(self.tokenizer,self.code_length,'',answer_text,question)\n",
    "        #inputs = torch.stack([ctx.convert()]).to(device)\n",
    "        #token_ids = tokenizer(question,answer_text,max_length=self.code_length,padding=\"max_length\")['input_ids']\n",
    "        #outputs = self.forward(inputs)\n",
    "        outputs = label.convert()\n",
    "        #print(outputs)\n",
    "        token_ids = tokenizer(label.CQu.question,label.CQu.context,max_length=self.code_length,padding=\"max_length\")['input_ids']\n",
    "        start_scores = outputs[0,:]\n",
    "        end_scores = outputs[1,:]\n",
    "        #print(start_scores,end_scores)\n",
    "        no_ans_scores = start_scores[-1] + end_scores[-1]\n",
    "        answer = ''\n",
    "        tokens = tokenizer.convert_ids_to_tokens(token_ids)\n",
    "        #print(outputs.logits_start.reshape(-1))\n",
    "        #print(outputs.logits_end.reshape(-1))\n",
    "        if (torch.max(start_scores) + torch.max(end_scores) <= no_ans_scores +  self.epsilon):\n",
    "            answer = ''\n",
    "        else:\n",
    "            answer_start = torch.argmax(start_scores)\n",
    "            answer_end = torch.argmax(end_scores)\n",
    "            answer = self.tokenizer.decode(token_ids[answer_start:answer_end+1])\n",
    "        return answer\n",
    "    def load_data(self,file_path):\n",
    "        CQu, La = load_data(file_path,self.tokenizer,self.code_length)\n",
    "        self.inputs = torch.stack([ctx.convert() for ctx in CQu])\n",
    "        self.labels = torch.stack([ctx.convert() for ctx in La])\n",
    "    \n",
    "    def train(self, epochs=3, batch_size = 48, lr = 3e-5, T=10):\n",
    "        optimizer = optim.Adam(self.parameters(), lr=lr)\n",
    "        criterion = CrossEntropy().to(device)\n",
    "        self.train_loader = DataLoader(TensorDataset(self.inputs,self.labels),batch_size = batch_size, shuffle = True, num_workers = 2)\n",
    "        epoch_loss = []\n",
    "        for epoch in range(epochs):      \n",
    "            running_loss = 0.0\n",
    "            epoch_running_loss = 0.0\n",
    "            batch_count = 0\n",
    "            for batchidx, (x, label) in enumerate(self.train_loader):\n",
    "                x, label = x.to(device), label.to(device)\n",
    "                output = self.forward(x)\n",
    "                output.logits_start=output.logits_start.resize(len(output.logits_start),self.code_length)\n",
    "                output.logits_end=output.logits_end.resize(len(output.logits_end),self.code_length)\n",
    "                loss = criterion(output, label)\n",
    "                # backprop\n",
    "                optimizer.zero_grad()  #梯度清0\n",
    "                loss.backward()   #梯度反传\n",
    "                optimizer.step()   #保留梯度\n",
    "\n",
    "                running_loss += loss.item()\n",
    "                epoch_running_loss += loss.item()\n",
    "                batch_count += 1\n",
    "                if batchidx % T == T-1:\n",
    "                    print(epoch,' batchidx: ', batchidx, ' loss: ', running_loss/T)\n",
    "                    running_loss = 0.0\n",
    "            epoch_loss.append(epoch_running_loss/batch_count)\n",
    "            print(epoch, 'loss:', epoch_running_loss/batch_count)\n",
    "        return epoch_loss\n",
    "    def printLabel(self,file_path):\n",
    "        CQu, La = load_data(file_path,self.tokenizer,self.code_length)\n",
    "        count = 0\n",
    "        precise = 0\n",
    "        for label in La:\n",
    "            count += 1\n",
    "            predict = self.predictionLabel(label)\n",
    "            if predict==label.text:\n",
    "                precise += 1\n",
    "            #else:\n",
    "            #    print(predict,label.text,label.has_ans)\n",
    "        print(precise/count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "50ec9d62-0604-4eed-83c3-8b6e1aeaa123",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9840725746875092\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda')\n",
    "net = BERTQuA(pre_trained_model,tokenizer,512,device)\n",
    "net.load_data('classifySimBertCluster2.json')\n",
    "net.printLabel('classifySimBertCluster2.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "48ba42be-008a-4086-a94b-9175f2f4c8b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/lib/python3.8/site-packages/torch/_tensor.py:493: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0  batchidx:  9  loss:  7.832815790176392\n",
      "0  batchidx:  19  loss:  6.6013837337493895\n",
      "0  batchidx:  29  loss:  5.789677381515503\n",
      "0  batchidx:  39  loss:  5.4132115840911865\n",
      "0  batchidx:  49  loss:  5.117201662063598\n",
      "0  batchidx:  59  loss:  4.79096667766571\n",
      "0  batchidx:  69  loss:  3.9224693775177\n",
      "0  batchidx:  79  loss:  3.638869047164917\n",
      "0  batchidx:  89  loss:  3.5030776977539064\n",
      "0  batchidx:  99  loss:  3.11249897480011\n",
      "0  batchidx:  109  loss:  2.8518182277679442\n",
      "0  batchidx:  119  loss:  3.2334677815437316\n",
      "0  batchidx:  129  loss:  2.6879336833953857\n",
      "0  batchidx:  139  loss:  2.994974064826965\n",
      "0  batchidx:  149  loss:  2.6096715688705445\n",
      "0  batchidx:  159  loss:  2.764068031311035\n",
      "0  batchidx:  169  loss:  2.819508504867554\n",
      "0  batchidx:  179  loss:  2.853278648853302\n",
      "0  batchidx:  189  loss:  2.9397136211395263\n",
      "0  batchidx:  199  loss:  2.7830420255661013\n",
      "0  batchidx:  209  loss:  2.6861269235610963\n",
      "0  batchidx:  219  loss:  3.2049712419509886\n",
      "0  batchidx:  229  loss:  2.6389445066452026\n",
      "0  batchidx:  239  loss:  2.520454728603363\n",
      "0  batchidx:  249  loss:  2.705828905105591\n",
      "0  batchidx:  259  loss:  2.469026803970337\n",
      "0  batchidx:  269  loss:  2.195827841758728\n",
      "0  batchidx:  279  loss:  2.0448460221290587\n",
      "0  batchidx:  289  loss:  2.689087748527527\n",
      "0  batchidx:  299  loss:  2.099853217601776\n",
      "0  batchidx:  309  loss:  2.4444473505020143\n",
      "0  batchidx:  319  loss:  2.5155341267585754\n",
      "0  batchidx:  329  loss:  2.2569949865341186\n",
      "0  batchidx:  339  loss:  2.3014026284217834\n",
      "0  batchidx:  349  loss:  2.1050559997558596\n",
      "0  batchidx:  359  loss:  2.4629257678985597\n",
      "0  batchidx:  369  loss:  2.1536381363868715\n",
      "0  batchidx:  379  loss:  2.2217095375061033\n",
      "0  batchidx:  389  loss:  1.8620466589927673\n",
      "0  batchidx:  399  loss:  2.406271529197693\n",
      "0  batchidx:  409  loss:  2.1937669277191163\n",
      "0  batchidx:  419  loss:  2.148220682144165\n",
      "0  batchidx:  429  loss:  2.014888644218445\n",
      "0  batchidx:  439  loss:  2.1972832918167113\n",
      "0  batchidx:  449  loss:  2.4035967350006104\n",
      "0  batchidx:  459  loss:  1.9085345268249512\n",
      "0  batchidx:  469  loss:  2.343782567977905\n",
      "0  batchidx:  479  loss:  2.3182343363761904\n",
      "0  batchidx:  489  loss:  2.15031498670578\n",
      "0  batchidx:  499  loss:  2.3624257326126097\n",
      "0  batchidx:  509  loss:  2.2427042603492735\n",
      "0  batchidx:  519  loss:  2.255663204193115\n",
      "0  batchidx:  529  loss:  2.2509774565696716\n",
      "0  batchidx:  539  loss:  2.1118236780166626\n",
      "0  batchidx:  549  loss:  2.1031389236450195\n",
      "0  batchidx:  559  loss:  2.104224717617035\n",
      "0  batchidx:  569  loss:  2.088292586803436\n",
      "0  batchidx:  579  loss:  2.091585099697113\n",
      "0  batchidx:  589  loss:  2.3239282369613647\n",
      "0  batchidx:  599  loss:  2.3356035113334657\n",
      "0  batchidx:  609  loss:  1.9995596408843994\n",
      "0  batchidx:  619  loss:  2.0529319643974304\n",
      "0  batchidx:  629  loss:  1.995360541343689\n",
      "0  batchidx:  639  loss:  2.061642622947693\n",
      "0  batchidx:  649  loss:  1.9842349767684937\n",
      "0  batchidx:  659  loss:  2.105049657821655\n",
      "0  batchidx:  669  loss:  1.9259014844894409\n",
      "0  batchidx:  679  loss:  1.8805370688438416\n",
      "0  batchidx:  689  loss:  1.972280466556549\n",
      "0  batchidx:  699  loss:  2.16351113319397\n",
      "0  batchidx:  709  loss:  2.071756291389465\n",
      "0  batchidx:  719  loss:  2.1915674209594727\n",
      "0  batchidx:  729  loss:  1.9816814661026\n",
      "0  batchidx:  739  loss:  2.086876404285431\n",
      "0  batchidx:  749  loss:  2.015633201599121\n",
      "0  batchidx:  759  loss:  1.8943269848823547\n",
      "0  batchidx:  769  loss:  1.9857945561408996\n",
      "0  batchidx:  779  loss:  2.108632493019104\n",
      "0  batchidx:  789  loss:  1.8598591923713683\n",
      "0  batchidx:  799  loss:  2.065098536014557\n",
      "0  batchidx:  809  loss:  1.8055140495300293\n",
      "0  batchidx:  819  loss:  1.9080782413482666\n",
      "0  batchidx:  829  loss:  1.6256437659263612\n",
      "0  batchidx:  839  loss:  1.89997798204422\n",
      "0  batchidx:  849  loss:  1.819924247264862\n",
      "0  batchidx:  859  loss:  1.9179942727088928\n",
      "0  batchidx:  869  loss:  1.7777318477630615\n",
      "0  batchidx:  879  loss:  1.9467227458953857\n",
      "0  batchidx:  889  loss:  1.775786817073822\n",
      "0  batchidx:  899  loss:  1.6134592056274415\n",
      "0  batchidx:  909  loss:  1.854798173904419\n",
      "0  batchidx:  919  loss:  1.7784512758255004\n",
      "0  batchidx:  929  loss:  2.1069809913635256\n",
      "0  batchidx:  939  loss:  1.6421624779701234\n",
      "0  batchidx:  949  loss:  1.84450204372406\n",
      "0  batchidx:  959  loss:  1.775888979434967\n",
      "0  batchidx:  969  loss:  1.7038981795310975\n",
      "0  batchidx:  979  loss:  1.8276459217071532\n",
      "0  batchidx:  989  loss:  1.8556349635124207\n",
      "0  batchidx:  999  loss:  1.6968062460422515\n",
      "0  batchidx:  1009  loss:  1.6069506645202636\n",
      "0  batchidx:  1019  loss:  1.9631837606430054\n",
      "0  batchidx:  1029  loss:  2.0593068599700928\n",
      "0  batchidx:  1039  loss:  1.8088046789169312\n",
      "0  batchidx:  1049  loss:  2.106734538078308\n",
      "0  batchidx:  1059  loss:  1.532002902030945\n",
      "0  batchidx:  1069  loss:  1.9556376934051514\n",
      "0  batchidx:  1079  loss:  1.9901368379592896\n",
      "0  batchidx:  1089  loss:  1.7944066762924193\n",
      "0  batchidx:  1099  loss:  1.6550533294677734\n",
      "0  batchidx:  1109  loss:  1.8972776651382446\n",
      "0  batchidx:  1119  loss:  1.925041925907135\n",
      "0  batchidx:  1129  loss:  1.9821172416210175\n",
      "0  batchidx:  1139  loss:  1.8160527706146241\n",
      "0  batchidx:  1149  loss:  1.4818890929222106\n",
      "0  batchidx:  1159  loss:  1.7886852264404296\n",
      "0  batchidx:  1169  loss:  1.777326261997223\n",
      "0  batchidx:  1179  loss:  2.0299798130989073\n",
      "0  batchidx:  1189  loss:  1.5492064118385316\n",
      "0  batchidx:  1199  loss:  1.3781406044960023\n",
      "0  batchidx:  1209  loss:  1.9278756380081177\n",
      "0  batchidx:  1219  loss:  1.6142974138259887\n",
      "0  batchidx:  1229  loss:  1.6874873161315918\n",
      "0  batchidx:  1239  loss:  1.8863336563110351\n",
      "0  batchidx:  1249  loss:  1.9820971488952637\n",
      "0  batchidx:  1259  loss:  1.6192275285720825\n",
      "0  batchidx:  1269  loss:  1.5112332940101623\n",
      "0  batchidx:  1279  loss:  1.845179557800293\n",
      "0  batchidx:  1289  loss:  2.141687572002411\n",
      "0  batchidx:  1299  loss:  1.7430270612239838\n",
      "0  batchidx:  1309  loss:  1.979266208410263\n",
      "0  batchidx:  1319  loss:  1.7368658781051636\n",
      "0  batchidx:  1329  loss:  1.6548664212226867\n",
      "0  batchidx:  1339  loss:  1.7538294792175293\n",
      "0  batchidx:  1349  loss:  1.8616802096366882\n",
      "0  batchidx:  1359  loss:  1.7078165650367736\n",
      "0  batchidx:  1369  loss:  1.8543810367584228\n",
      "0  batchidx:  1379  loss:  1.851981484889984\n",
      "0  batchidx:  1389  loss:  1.7228593826293945\n",
      "0  batchidx:  1399  loss:  1.7037424445152283\n",
      "0  batchidx:  1409  loss:  1.8253581881523133\n",
      "0  batchidx:  1419  loss:  1.8800449132919312\n",
      "0  batchidx:  1429  loss:  1.669223153591156\n",
      "0  batchidx:  1439  loss:  1.931159770488739\n",
      "0  batchidx:  1449  loss:  1.5630654692649841\n",
      "0  batchidx:  1459  loss:  1.9263655424118042\n",
      "0  batchidx:  1469  loss:  1.9157074809074401\n",
      "0  batchidx:  1479  loss:  1.6456506609916688\n",
      "0  batchidx:  1489  loss:  1.640211045742035\n",
      "0  batchidx:  1499  loss:  1.6513904809951783\n",
      "0  batchidx:  1509  loss:  2.004265320301056\n",
      "0  batchidx:  1519  loss:  1.8179661273956298\n",
      "0  batchidx:  1529  loss:  1.6369842052459718\n",
      "0  batchidx:  1539  loss:  1.7457335710525512\n",
      "0  batchidx:  1549  loss:  1.710051143169403\n",
      "0  batchidx:  1559  loss:  1.6672343254089355\n",
      "0  batchidx:  1569  loss:  1.541299283504486\n",
      "0  batchidx:  1579  loss:  1.681356966495514\n",
      "0  batchidx:  1589  loss:  1.8598562955856324\n",
      "0  batchidx:  1599  loss:  1.8005101799964904\n",
      "0  batchidx:  1609  loss:  2.0309056520462034\n",
      "0  batchidx:  1619  loss:  1.6736908435821534\n",
      "0  batchidx:  1629  loss:  1.482788187265396\n",
      "0  batchidx:  1639  loss:  1.6431422710418702\n",
      "0  batchidx:  1649  loss:  2.047778105735779\n",
      "0  batchidx:  1659  loss:  1.7249351143836975\n",
      "0  batchidx:  1669  loss:  1.683067011833191\n",
      "0  batchidx:  1679  loss:  2.169436454772949\n",
      "0  batchidx:  1689  loss:  1.5166615128517151\n",
      "0  batchidx:  1699  loss:  1.8228608965873718\n",
      "0  batchidx:  1709  loss:  2.087976408004761\n",
      "0  batchidx:  1719  loss:  1.8243005514144897\n",
      "0  batchidx:  1729  loss:  1.6920487105846405\n",
      "0  batchidx:  1739  loss:  1.724137532711029\n",
      "0  batchidx:  1749  loss:  1.6218907833099365\n",
      "0  batchidx:  1759  loss:  1.7836854219436646\n",
      "0  batchidx:  1769  loss:  1.498724377155304\n",
      "0  batchidx:  1779  loss:  1.4804720818996429\n",
      "0  batchidx:  1789  loss:  1.5596838355064393\n",
      "0  batchidx:  1799  loss:  1.9074051022529601\n",
      "0  batchidx:  1809  loss:  1.6256202280521392\n",
      "0  batchidx:  1819  loss:  1.451561403274536\n",
      "0  batchidx:  1829  loss:  1.7686603784561157\n",
      "0  batchidx:  1839  loss:  1.8077430129051208\n",
      "0  batchidx:  1849  loss:  1.4439790070056915\n",
      "0  batchidx:  1859  loss:  1.6599865078926086\n",
      "0  batchidx:  1869  loss:  1.6953061461448669\n",
      "0  batchidx:  1879  loss:  1.3406772375106812\n",
      "0  batchidx:  1889  loss:  1.4007007718086242\n",
      "0  batchidx:  1899  loss:  1.4832836389541626\n",
      "0  batchidx:  1909  loss:  1.7718494892120362\n",
      "0  batchidx:  1919  loss:  1.8152875065803529\n",
      "0  batchidx:  1929  loss:  1.9584886074066161\n",
      "0  batchidx:  1939  loss:  1.5840330302715302\n",
      "0  batchidx:  1949  loss:  1.6799299001693726\n",
      "0  batchidx:  1959  loss:  1.5890162587165833\n",
      "0  batchidx:  1969  loss:  1.4757210731506347\n",
      "0  batchidx:  1979  loss:  1.5687666654586792\n",
      "0  batchidx:  1989  loss:  1.4347772121429443\n",
      "0  batchidx:  1999  loss:  1.4182658910751342\n",
      "0  batchidx:  2009  loss:  1.4584807842969894\n",
      "0  batchidx:  2019  loss:  1.4254297316074371\n",
      "0  batchidx:  2029  loss:  1.566952395439148\n",
      "0  batchidx:  2039  loss:  1.4415004014968873\n",
      "0  batchidx:  2049  loss:  1.6403164625167848\n",
      "0  batchidx:  2059  loss:  1.4654796481132508\n",
      "0  batchidx:  2069  loss:  1.453356385231018\n",
      "0  batchidx:  2079  loss:  1.5676597237586976\n",
      "0  batchidx:  2089  loss:  1.7185939133167267\n",
      "0  batchidx:  2099  loss:  1.7558252215385437\n",
      "0  batchidx:  2109  loss:  1.7064422965049744\n",
      "0 loss: 2.0738220441741393\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "1  batchidx:  9  loss:  1.0236998438835143\n",
      "1  batchidx:  19  loss:  0.9443032920360566\n",
      "1  batchidx:  29  loss:  0.8864522218704224\n",
      "1  batchidx:  39  loss:  1.428597989678383\n",
      "1  batchidx:  49  loss:  1.0664264738559723\n",
      "1  batchidx:  59  loss:  1.150032675266266\n",
      "1  batchidx:  69  loss:  0.8840330004692077\n",
      "1  batchidx:  79  loss:  1.1128244340419768\n",
      "1  batchidx:  89  loss:  0.9647180497646332\n",
      "1  batchidx:  99  loss:  1.040446299314499\n",
      "1  batchidx:  109  loss:  0.9354940861463547\n",
      "1  batchidx:  119  loss:  0.8242304682731628\n",
      "1  batchidx:  129  loss:  0.7881461024284363\n",
      "1  batchidx:  139  loss:  0.8943664371967316\n",
      "1  batchidx:  149  loss:  1.0594694793224335\n",
      "1  batchidx:  159  loss:  0.8769783318042755\n",
      "1  batchidx:  169  loss:  1.3393062353134155\n",
      "1  batchidx:  179  loss:  0.9714162409305572\n",
      "1  batchidx:  189  loss:  1.1910255908966065\n",
      "1  batchidx:  199  loss:  1.1658748030662536\n",
      "1  batchidx:  209  loss:  0.7459094166755676\n",
      "1  batchidx:  219  loss:  0.9727014303207397\n",
      "1  batchidx:  229  loss:  1.1982580959796905\n",
      "1  batchidx:  239  loss:  1.1058672308921813\n",
      "1  batchidx:  249  loss:  0.9128336250782013\n",
      "1  batchidx:  259  loss:  0.9298520356416702\n",
      "1  batchidx:  269  loss:  0.9364094138145447\n",
      "1  batchidx:  279  loss:  0.7230333119630814\n",
      "1  batchidx:  289  loss:  0.9906000316143035\n",
      "1  batchidx:  299  loss:  1.053600299358368\n",
      "1  batchidx:  309  loss:  0.7450345754623413\n",
      "1  batchidx:  319  loss:  1.3313092589378357\n",
      "1  batchidx:  329  loss:  1.1146281003952025\n",
      "1  batchidx:  339  loss:  0.9335955381393433\n",
      "1  batchidx:  349  loss:  0.8600117564201355\n",
      "1  batchidx:  359  loss:  0.6637619644403457\n",
      "1  batchidx:  369  loss:  0.9443695664405822\n",
      "1  batchidx:  379  loss:  1.0209281116724014\n",
      "1  batchidx:  389  loss:  1.0090506196022033\n",
      "1  batchidx:  399  loss:  0.9240889430046082\n",
      "1  batchidx:  409  loss:  0.9167734503746032\n",
      "1  batchidx:  419  loss:  1.0670220673084259\n",
      "1  batchidx:  429  loss:  0.9532724440097808\n",
      "1  batchidx:  439  loss:  1.1546141415834428\n",
      "1  batchidx:  449  loss:  1.0750602424144744\n",
      "1  batchidx:  459  loss:  1.114292275905609\n",
      "1  batchidx:  469  loss:  1.0731566846370697\n",
      "1  batchidx:  479  loss:  1.0681518971920014\n",
      "1  batchidx:  489  loss:  0.9624097228050232\n",
      "1  batchidx:  499  loss:  0.992407476902008\n",
      "1  batchidx:  509  loss:  1.1469869017601013\n",
      "1  batchidx:  519  loss:  1.091402405500412\n",
      "1  batchidx:  529  loss:  1.0254329323768616\n",
      "1  batchidx:  539  loss:  0.9211545169353486\n",
      "1  batchidx:  549  loss:  0.9655312478542328\n",
      "1  batchidx:  559  loss:  1.2543343424797058\n",
      "1  batchidx:  569  loss:  1.0232714653015136\n",
      "1  batchidx:  579  loss:  1.0545256435871124\n",
      "1  batchidx:  589  loss:  0.6510457068681716\n",
      "1  batchidx:  599  loss:  0.9901169240474701\n",
      "1  batchidx:  609  loss:  1.104117825627327\n",
      "1  batchidx:  619  loss:  0.9913866817951202\n",
      "1  batchidx:  629  loss:  0.9426952123641967\n",
      "1  batchidx:  639  loss:  1.204046607017517\n",
      "1  batchidx:  649  loss:  0.9846395432949067\n",
      "1  batchidx:  659  loss:  1.1463717699050904\n",
      "1  batchidx:  669  loss:  0.950654286146164\n",
      "1  batchidx:  679  loss:  1.164369809627533\n",
      "1  batchidx:  689  loss:  1.1585045993328094\n",
      "1  batchidx:  699  loss:  1.1622235357761384\n",
      "1  batchidx:  709  loss:  1.0587783455848694\n",
      "1  batchidx:  719  loss:  0.8527774482965469\n",
      "1  batchidx:  729  loss:  1.044710063934326\n",
      "1  batchidx:  739  loss:  1.0748660892248154\n",
      "1  batchidx:  749  loss:  1.3373883068561554\n",
      "1  batchidx:  759  loss:  0.8517140924930573\n",
      "1  batchidx:  769  loss:  1.0701840937137603\n",
      "1  batchidx:  779  loss:  0.9835505902767181\n",
      "1  batchidx:  789  loss:  0.7785116046667099\n",
      "1  batchidx:  799  loss:  0.9664656400680542\n",
      "1  batchidx:  809  loss:  1.1913678228855134\n",
      "1  batchidx:  819  loss:  0.9990988671779633\n",
      "1  batchidx:  829  loss:  0.8533414721488952\n",
      "1  batchidx:  839  loss:  0.9998514890670777\n",
      "1  batchidx:  849  loss:  0.8572728574275971\n",
      "1  batchidx:  859  loss:  1.2549358785152436\n",
      "1  batchidx:  869  loss:  1.0524463295936584\n",
      "1  batchidx:  879  loss:  0.890660572052002\n",
      "1  batchidx:  889  loss:  1.0574902534484862\n",
      "1  batchidx:  899  loss:  1.1600064575672149\n",
      "1  batchidx:  909  loss:  0.9154610306024551\n",
      "1  batchidx:  919  loss:  1.3853863179683685\n",
      "1  batchidx:  929  loss:  1.0028125107288361\n",
      "1  batchidx:  939  loss:  0.9343546956777573\n",
      "1  batchidx:  949  loss:  1.0767114222049714\n",
      "1  batchidx:  959  loss:  1.305069202184677\n",
      "1  batchidx:  969  loss:  1.041537567973137\n",
      "1  batchidx:  979  loss:  1.01633882522583\n",
      "1  batchidx:  989  loss:  0.9947724163532257\n",
      "1  batchidx:  999  loss:  0.8897556573152542\n",
      "1  batchidx:  1009  loss:  0.9329427599906921\n",
      "1  batchidx:  1019  loss:  0.9957180559635163\n",
      "1  batchidx:  1029  loss:  0.8654500007629394\n",
      "1  batchidx:  1039  loss:  1.0613773822784425\n",
      "1  batchidx:  1049  loss:  0.9781932353973388\n",
      "1  batchidx:  1059  loss:  1.000367397069931\n",
      "1  batchidx:  1069  loss:  1.0759026288986206\n",
      "1  batchidx:  1079  loss:  1.388983577489853\n",
      "1  batchidx:  1089  loss:  1.2577218890190125\n",
      "1  batchidx:  1099  loss:  1.138880267739296\n",
      "1  batchidx:  1109  loss:  1.1616252422332765\n",
      "1  batchidx:  1119  loss:  1.0183630526065826\n",
      "1  batchidx:  1129  loss:  1.1080848813056945\n",
      "1  batchidx:  1139  loss:  1.004224306344986\n",
      "1  batchidx:  1149  loss:  0.8484273076057434\n",
      "1  batchidx:  1159  loss:  1.1342608869075774\n",
      "1  batchidx:  1169  loss:  1.151831966638565\n",
      "1  batchidx:  1179  loss:  0.9936734676361084\n",
      "1  batchidx:  1189  loss:  0.8322814464569092\n",
      "1  batchidx:  1199  loss:  1.1108876436948776\n",
      "1  batchidx:  1209  loss:  1.2219884812831878\n",
      "1  batchidx:  1219  loss:  1.035044538974762\n",
      "1  batchidx:  1229  loss:  0.8483489632606507\n",
      "1  batchidx:  1239  loss:  0.9096889555454254\n",
      "1  batchidx:  1249  loss:  0.994076281785965\n",
      "1  batchidx:  1259  loss:  0.9362008839845657\n",
      "1  batchidx:  1269  loss:  1.1400014847517013\n",
      "1  batchidx:  1279  loss:  1.1512137591838836\n",
      "1  batchidx:  1289  loss:  0.9116641223430634\n",
      "1  batchidx:  1299  loss:  1.008849161863327\n",
      "1  batchidx:  1309  loss:  1.0829212844371796\n",
      "1  batchidx:  1319  loss:  0.7000547170639038\n",
      "1  batchidx:  1329  loss:  1.0507796108722687\n",
      "1  batchidx:  1339  loss:  1.034931629896164\n",
      "1  batchidx:  1349  loss:  1.06864293217659\n",
      "1  batchidx:  1359  loss:  0.8639537632465363\n",
      "1  batchidx:  1369  loss:  0.9635647296905517\n",
      "1  batchidx:  1379  loss:  0.8397333979606628\n",
      "1  batchidx:  1389  loss:  1.0047203361988069\n",
      "1  batchidx:  1399  loss:  1.0636130452156067\n",
      "1  batchidx:  1409  loss:  1.0020743250846862\n",
      "1  batchidx:  1419  loss:  0.9498702406883239\n",
      "1  batchidx:  1429  loss:  1.1344819128513337\n",
      "1  batchidx:  1439  loss:  1.104570609331131\n",
      "1  batchidx:  1449  loss:  1.0963115394115448\n",
      "1  batchidx:  1459  loss:  0.9547393321990967\n",
      "1  batchidx:  1469  loss:  1.1019793629646302\n",
      "1  batchidx:  1479  loss:  1.013293093442917\n",
      "1  batchidx:  1489  loss:  1.1085026800632476\n",
      "1  batchidx:  1499  loss:  1.1199582070112228\n",
      "1  batchidx:  1509  loss:  1.04753620326519\n",
      "1  batchidx:  1519  loss:  0.9550823330879211\n",
      "1  batchidx:  1529  loss:  1.076992267370224\n",
      "1  batchidx:  1539  loss:  1.1952254712581634\n",
      "1  batchidx:  1549  loss:  0.8688715726137162\n",
      "1  batchidx:  1559  loss:  0.9407260656356812\n",
      "1  batchidx:  1569  loss:  1.084270614385605\n",
      "1  batchidx:  1579  loss:  1.0406966149806975\n",
      "1  batchidx:  1589  loss:  0.850161275267601\n",
      "1  batchidx:  1599  loss:  1.1584482550621034\n",
      "1  batchidx:  1609  loss:  1.0561457931995393\n",
      "1  batchidx:  1619  loss:  0.941183602809906\n",
      "1  batchidx:  1629  loss:  1.044228768348694\n",
      "1  batchidx:  1639  loss:  1.19557181596756\n",
      "1  batchidx:  1649  loss:  1.1366798639297486\n",
      "1  batchidx:  1659  loss:  0.817877733707428\n",
      "1  batchidx:  1669  loss:  1.2942616403102876\n",
      "1  batchidx:  1679  loss:  1.3401543855667115\n",
      "1  batchidx:  1689  loss:  1.0703299283981322\n",
      "1  batchidx:  1699  loss:  0.9874302476644516\n",
      "1  batchidx:  1709  loss:  1.1139927506446838\n",
      "1  batchidx:  1719  loss:  0.9508727073669434\n",
      "1  batchidx:  1729  loss:  1.1127999365329742\n",
      "1  batchidx:  1739  loss:  1.1700466871261597\n",
      "1  batchidx:  1749  loss:  1.21116241812706\n",
      "1  batchidx:  1759  loss:  1.2014622211456298\n",
      "1  batchidx:  1769  loss:  1.2195080876350404\n",
      "1  batchidx:  1779  loss:  1.092856216430664\n",
      "1  batchidx:  1789  loss:  1.2920320630073547\n",
      "1  batchidx:  1799  loss:  1.1153518855571747\n",
      "1  batchidx:  1809  loss:  1.0878883719444274\n",
      "1  batchidx:  1819  loss:  0.9355419278144836\n",
      "1  batchidx:  1829  loss:  1.0047566771507264\n",
      "1  batchidx:  1839  loss:  1.0184372425079347\n",
      "1  batchidx:  1849  loss:  0.893883439898491\n",
      "1  batchidx:  1859  loss:  0.7985199093818665\n",
      "1  batchidx:  1869  loss:  0.9755292356014251\n",
      "1  batchidx:  1879  loss:  1.1179000794887544\n",
      "1  batchidx:  1889  loss:  0.9571181386709213\n",
      "1  batchidx:  1899  loss:  0.8576499164104462\n",
      "1  batchidx:  1909  loss:  1.0245560079813003\n",
      "1  batchidx:  1919  loss:  1.23440220952034\n",
      "1  batchidx:  1929  loss:  1.2355568170547486\n",
      "1  batchidx:  1939  loss:  1.0399523317813872\n",
      "1  batchidx:  1949  loss:  0.8885949552059174\n",
      "1  batchidx:  1959  loss:  0.74767856746912\n",
      "1  batchidx:  1969  loss:  1.1654237657785416\n",
      "1  batchidx:  1979  loss:  1.098984068632126\n",
      "1  batchidx:  1989  loss:  1.0952680170536042\n",
      "1  batchidx:  1999  loss:  1.170351469516754\n",
      "1  batchidx:  2009  loss:  1.189823219180107\n",
      "1  batchidx:  2019  loss:  0.9254689395427704\n",
      "1  batchidx:  2029  loss:  1.0101989448070525\n",
      "1  batchidx:  2039  loss:  1.0413945317268372\n",
      "1  batchidx:  2049  loss:  1.0669966876506805\n",
      "1  batchidx:  2059  loss:  1.120066726207733\n",
      "1  batchidx:  2069  loss:  0.8844573497772217\n",
      "1  batchidx:  2079  loss:  0.8818479001522064\n",
      "1  batchidx:  2089  loss:  0.8451322361826896\n",
      "1  batchidx:  2099  loss:  1.1966073364019394\n",
      "1  batchidx:  2109  loss:  1.4389217436313628\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "1 loss: 1.0327889681679314\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "2  batchidx:  9  loss:  0.6858038783073426\n",
      "2  batchidx:  19  loss:  0.4720914289355278\n",
      "2  batchidx:  29  loss:  0.32609731443226336\n",
      "2  batchidx:  39  loss:  0.36901583299040797\n",
      "2  batchidx:  49  loss:  0.319681191816926\n",
      "2  batchidx:  59  loss:  0.45853366479277613\n",
      "2  batchidx:  69  loss:  0.45133226066827775\n",
      "2  batchidx:  79  loss:  0.4826085031032562\n",
      "2  batchidx:  89  loss:  0.31207899153232577\n",
      "2  batchidx:  99  loss:  0.42326628640294073\n",
      "2  batchidx:  109  loss:  0.37196471393108366\n",
      "2  batchidx:  119  loss:  0.4318356543779373\n",
      "2  batchidx:  129  loss:  0.3436154708266258\n",
      "2  batchidx:  139  loss:  0.29704878479242325\n",
      "2  batchidx:  149  loss:  0.313847329467535\n",
      "2  batchidx:  159  loss:  0.30396935641765593\n",
      "2  batchidx:  169  loss:  0.489933542907238\n",
      "2  batchidx:  179  loss:  0.5375071004033088\n",
      "2  batchidx:  189  loss:  0.5438169181346894\n",
      "2  batchidx:  199  loss:  0.45325733050704003\n",
      "2  batchidx:  209  loss:  0.3545614004135132\n",
      "2  batchidx:  219  loss:  0.47099391669034957\n",
      "2  batchidx:  229  loss:  0.35923080891370773\n",
      "2  batchidx:  239  loss:  0.4470984131097794\n",
      "2  batchidx:  249  loss:  0.6054579332470894\n",
      "2  batchidx:  259  loss:  0.5450908720493317\n",
      "2  batchidx:  269  loss:  0.3885058328509331\n",
      "2  batchidx:  279  loss:  0.4743921831250191\n",
      "2  batchidx:  289  loss:  0.571553087234497\n",
      "2  batchidx:  299  loss:  0.3088191419839859\n",
      "2  batchidx:  309  loss:  0.9250105142593383\n",
      "2  batchidx:  319  loss:  0.6537904955446721\n",
      "2  batchidx:  329  loss:  0.5972235485911369\n",
      "2  batchidx:  339  loss:  0.3975259780883789\n",
      "2  batchidx:  349  loss:  0.45666472613811493\n",
      "2  batchidx:  359  loss:  0.3308083314448595\n",
      "2  batchidx:  369  loss:  0.4703786760568619\n",
      "2  batchidx:  379  loss:  0.5221798777580261\n",
      "2  batchidx:  389  loss:  0.4526951849460602\n",
      "2  batchidx:  399  loss:  0.5699953436851501\n",
      "2  batchidx:  409  loss:  0.4561513736844063\n",
      "2  batchidx:  419  loss:  0.5387340739369393\n",
      "2  batchidx:  429  loss:  0.5373818382620812\n",
      "2  batchidx:  439  loss:  0.488452160358429\n",
      "2  batchidx:  449  loss:  0.36659657657146455\n",
      "2  batchidx:  459  loss:  0.5747480496764183\n",
      "2  batchidx:  469  loss:  0.5142743170261384\n",
      "2  batchidx:  479  loss:  0.3625312209129333\n",
      "2  batchidx:  489  loss:  0.48048749268054963\n",
      "2  batchidx:  499  loss:  0.4897403120994568\n",
      "2  batchidx:  509  loss:  0.5819967240095139\n",
      "2  batchidx:  519  loss:  0.612451146543026\n",
      "2  batchidx:  529  loss:  0.4373063281178474\n",
      "2  batchidx:  539  loss:  0.39081198871135714\n",
      "2  batchidx:  549  loss:  0.4456050030887127\n",
      "2  batchidx:  559  loss:  0.620352354645729\n",
      "2  batchidx:  569  loss:  0.4587560027837753\n",
      "2  batchidx:  579  loss:  0.3734859675168991\n",
      "2  batchidx:  589  loss:  0.3294348046183586\n",
      "2  batchidx:  599  loss:  0.4502679739147425\n",
      "2  batchidx:  609  loss:  0.4970329761505127\n",
      "2  batchidx:  619  loss:  0.5898903697729111\n",
      "2  batchidx:  629  loss:  0.40949576199054716\n",
      "2  batchidx:  639  loss:  0.5816661655902863\n",
      "2  batchidx:  649  loss:  0.5946819070726633\n",
      "2  batchidx:  659  loss:  0.4863764762878418\n",
      "2  batchidx:  669  loss:  0.5144458770751953\n",
      "2  batchidx:  679  loss:  0.35636438205838206\n",
      "2  batchidx:  689  loss:  0.7142133481800557\n",
      "2  batchidx:  699  loss:  0.4188272461295128\n",
      "2  batchidx:  709  loss:  0.37250197380781175\n",
      "2  batchidx:  719  loss:  0.5081468313932419\n",
      "2  batchidx:  729  loss:  0.44073741883039474\n",
      "2  batchidx:  739  loss:  0.5810182794928551\n",
      "2  batchidx:  749  loss:  0.5908831268548965\n",
      "2  batchidx:  759  loss:  0.4752479150891304\n",
      "2  batchidx:  769  loss:  0.570894080400467\n",
      "2  batchidx:  779  loss:  0.5972578436136246\n",
      "2  batchidx:  789  loss:  0.5868655472993851\n",
      "2  batchidx:  799  loss:  0.5980218946933746\n",
      "2  batchidx:  809  loss:  0.5643034122884274\n",
      "2  batchidx:  819  loss:  0.600071108341217\n",
      "2  batchidx:  829  loss:  0.3372414067387581\n",
      "2  batchidx:  839  loss:  0.3007834076881409\n",
      "2  batchidx:  849  loss:  0.5342311419546604\n",
      "2  batchidx:  859  loss:  0.4910777807235718\n",
      "2  batchidx:  869  loss:  0.594045703113079\n",
      "2  batchidx:  879  loss:  0.7435335755348206\n",
      "2  batchidx:  889  loss:  0.49456157386302946\n",
      "2  batchidx:  899  loss:  0.42310103476047517\n",
      "2  batchidx:  909  loss:  0.7126790672540665\n",
      "2  batchidx:  919  loss:  0.5055494368076324\n",
      "2  batchidx:  929  loss:  0.3795788124203682\n",
      "2  batchidx:  939  loss:  0.4976709306240082\n",
      "2  batchidx:  949  loss:  0.7967014342546463\n",
      "2  batchidx:  959  loss:  0.5361684545874595\n",
      "2  batchidx:  969  loss:  0.6333866968750954\n",
      "2  batchidx:  979  loss:  0.5257017560303211\n",
      "2  batchidx:  989  loss:  0.4309471890330315\n",
      "2  batchidx:  999  loss:  0.647888720035553\n",
      "2  batchidx:  1009  loss:  0.5916466057300568\n",
      "2  batchidx:  1019  loss:  0.5340976431965828\n",
      "2  batchidx:  1029  loss:  0.6545641332864761\n",
      "2  batchidx:  1039  loss:  0.6531667917966842\n",
      "2  batchidx:  1049  loss:  0.5841976970434188\n",
      "2  batchidx:  1059  loss:  0.5329318806529045\n",
      "2  batchidx:  1069  loss:  0.39428176581859586\n",
      "2  batchidx:  1079  loss:  0.46678672879934313\n",
      "2  batchidx:  1089  loss:  0.5071933522820473\n",
      "2  batchidx:  1099  loss:  0.38720695190131665\n",
      "2  batchidx:  1109  loss:  0.5504657745361328\n",
      "2  batchidx:  1119  loss:  0.4474598795175552\n",
      "2  batchidx:  1129  loss:  0.5056652382016182\n",
      "2  batchidx:  1139  loss:  0.5936351269483566\n",
      "2  batchidx:  1149  loss:  0.6496783465147018\n",
      "2  batchidx:  1159  loss:  0.5658542573451996\n",
      "2  batchidx:  1169  loss:  0.7535816103219986\n",
      "2  batchidx:  1179  loss:  0.5407021954655647\n",
      "2  batchidx:  1189  loss:  0.46952094286680224\n",
      "2  batchidx:  1199  loss:  0.6958224028348923\n",
      "2  batchidx:  1209  loss:  0.6339251846075058\n",
      "2  batchidx:  1219  loss:  0.6622029483318329\n",
      "2  batchidx:  1229  loss:  0.6775611698627472\n",
      "2  batchidx:  1239  loss:  0.54169050604105\n",
      "2  batchidx:  1249  loss:  0.6310642659664154\n",
      "2  batchidx:  1259  loss:  0.5403922528028489\n",
      "2  batchidx:  1269  loss:  0.43528171479701994\n",
      "2  batchidx:  1279  loss:  0.3979607574641705\n",
      "2  batchidx:  1289  loss:  0.35045474767684937\n",
      "2  batchidx:  1299  loss:  0.8316734790802002\n",
      "2  batchidx:  1309  loss:  0.5404838234186172\n",
      "2  batchidx:  1319  loss:  0.5623455613851547\n",
      "2  batchidx:  1329  loss:  0.42386514097452166\n",
      "2  batchidx:  1339  loss:  0.5009933121502399\n",
      "2  batchidx:  1349  loss:  0.5446516007184983\n",
      "2  batchidx:  1359  loss:  0.5607128739356995\n",
      "2  batchidx:  1369  loss:  0.5427887797355652\n",
      "2  batchidx:  1379  loss:  0.3590733900666237\n",
      "2  batchidx:  1389  loss:  0.5140660412609577\n",
      "2  batchidx:  1399  loss:  0.5234698951244354\n",
      "2  batchidx:  1409  loss:  0.5675306886434555\n",
      "2  batchidx:  1419  loss:  0.5062008380889893\n",
      "2  batchidx:  1429  loss:  0.6024700537323951\n",
      "2  batchidx:  1439  loss:  0.7648227050900459\n",
      "2  batchidx:  1449  loss:  0.49345511198043823\n",
      "2  batchidx:  1459  loss:  0.41065618544816973\n",
      "2  batchidx:  1469  loss:  0.48114149272441864\n",
      "2  batchidx:  1479  loss:  0.5132683545351029\n",
      "2  batchidx:  1489  loss:  0.5648992322385311\n",
      "2  batchidx:  1499  loss:  0.47951907217502593\n",
      "2  batchidx:  1509  loss:  0.5909839123487473\n",
      "2  batchidx:  1519  loss:  0.5826916933059693\n",
      "2  batchidx:  1529  loss:  0.5180770263075829\n",
      "2  batchidx:  1539  loss:  0.39415100440382955\n",
      "2  batchidx:  1549  loss:  0.46927280575037\n",
      "2  batchidx:  1559  loss:  0.4557793319225311\n",
      "2  batchidx:  1569  loss:  0.6423281580209732\n",
      "2  batchidx:  1579  loss:  0.6608300089836121\n",
      "2  batchidx:  1589  loss:  0.6178293704986573\n",
      "2  batchidx:  1599  loss:  0.6183746039867402\n",
      "2  batchidx:  1609  loss:  0.4802839130163193\n",
      "2  batchidx:  1619  loss:  0.5909715443849564\n",
      "2  batchidx:  1629  loss:  0.7118044912815094\n",
      "2  batchidx:  1639  loss:  0.6304609537124634\n",
      "2  batchidx:  1649  loss:  0.5358462870121002\n",
      "2  batchidx:  1659  loss:  0.5030326806008816\n",
      "2  batchidx:  1669  loss:  0.6856432169675827\n",
      "2  batchidx:  1679  loss:  0.5431256979703903\n",
      "2  batchidx:  1689  loss:  0.47119817733764646\n",
      "2  batchidx:  1699  loss:  0.6592087253928185\n",
      "2  batchidx:  1709  loss:  0.6675917327404022\n",
      "2  batchidx:  1719  loss:  0.4032387837767601\n",
      "2  batchidx:  1729  loss:  0.6566881865262986\n",
      "2  batchidx:  1739  loss:  0.5624670267105103\n",
      "2  batchidx:  1749  loss:  0.44851969331502917\n",
      "2  batchidx:  1759  loss:  0.5981195092201232\n",
      "2  batchidx:  1769  loss:  0.7223409682512283\n",
      "2  batchidx:  1779  loss:  0.54655771702528\n",
      "2  batchidx:  1789  loss:  0.5506032973527908\n",
      "2  batchidx:  1799  loss:  0.5427429482340813\n",
      "2  batchidx:  1809  loss:  0.7210158243775368\n",
      "2  batchidx:  1819  loss:  0.5086872100830078\n",
      "2  batchidx:  1829  loss:  0.5059671133756638\n",
      "2  batchidx:  1839  loss:  0.6237295545637608\n",
      "2  batchidx:  1849  loss:  0.5368751466274262\n",
      "2  batchidx:  1859  loss:  0.43879102542996407\n",
      "2  batchidx:  1869  loss:  0.3961869701743126\n",
      "2  batchidx:  1879  loss:  0.8004021346569061\n",
      "2  batchidx:  1889  loss:  0.805799876153469\n",
      "2  batchidx:  1899  loss:  0.44380990266799925\n",
      "2  batchidx:  1909  loss:  0.5397788137197495\n",
      "2  batchidx:  1919  loss:  0.7486755758523941\n",
      "2  batchidx:  1929  loss:  0.59542216360569\n",
      "2  batchidx:  1939  loss:  0.6741252169013023\n",
      "2  batchidx:  1949  loss:  0.5666327476501465\n",
      "2  batchidx:  1959  loss:  0.5361189238727093\n",
      "2  batchidx:  1969  loss:  0.5904875278472901\n",
      "2  batchidx:  1979  loss:  0.7547908648848534\n",
      "2  batchidx:  1989  loss:  0.5888871192932129\n",
      "2  batchidx:  1999  loss:  0.4708729088306427\n",
      "2  batchidx:  2009  loss:  0.4523389331996441\n",
      "2  batchidx:  2019  loss:  0.6277111977338791\n",
      "2  batchidx:  2029  loss:  0.7845647096633911\n",
      "2  batchidx:  2039  loss:  0.6432738780975342\n",
      "2  batchidx:  2049  loss:  0.7346396163105965\n",
      "2  batchidx:  2059  loss:  0.6747593387961388\n",
      "2  batchidx:  2069  loss:  0.6499813616275787\n",
      "2  batchidx:  2079  loss:  0.5494979918003082\n",
      "2  batchidx:  2089  loss:  0.46763084828853607\n",
      "2  batchidx:  2099  loss:  0.5658609449863434\n",
      "2  batchidx:  2109  loss:  0.5520246416330338\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "2 loss: 0.5310837193647494\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "60"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "epoch_loss = net.train(batch_size=16,epochs=3)\n",
    "f = open(\"loss_cluster2.txt\", \"w\")\n",
    "f.write(str(epoch_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f9ec2a11-a4cc-4cba-89fd-70f22ebe0965",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question = \"\"\"Which journalist considered Spectre the worst James Bond movie in three decades?\"\"\"\n",
    "context = \"\"\"Critical appraisal of the film was mixed in the United States. In a lukewarm review for RogerEbert.com, Matt Zoller Seitz gave the film 2.5 stars out of 4, describing Spectre as inconsistent and unable to capitalise on its potential. Kenneth Turan, reviewing the film for Los Angeles Times, concluded that Spectre \\\"comes off as exhausted and uninspired\\\". Manohla Dargis of The New York Times panned the film as having \\\"nothing surprising\\\" and sacrificing its originality for the sake of box office returns. Forbes' Scott Mendelson also heavily criticised the film, denouncing Spectre as \\\"the worst 007 movie in 30 years\\\". Darren Franich of Entertainment Weekly viewed Spectre as \\\"an overreaction to our current blockbuster moment\\\", aspiring \\\"to be a serialized sequel\\\" and proving \\\"itself as a Saga\\\". While noting that \\\"[n]othing that happens in Spectre holds up to even minor logical scrutiny\\\", he had \\\"come not to bury Spectre, but to weirdly praise it. Because the final act of the movie is so strange, so willfully obtuse, that it deserves extra attention.\\\" In a positive review Rolling Stone, Peter Travers gave the film 3.5 stars out of 4, describing \\\"The 24th movie about the British MI6 agent with a license to kill is party time for Bond fans, a fierce, funny, gorgeously produced valentine to the longest-running franchise in movies\\\". Other positive reviews from Mick LaSalle from the San Francisco Chronicle, gave it a perfect 100 score, stating: \\u201cOne of the great satisfactions of Spectre is that, in addition to all the stirring action, and all the timely references to a secret organization out to steal everyone\\u2019s personal information, we get to believe in Bond as a person.\\u201d Stephen Whitty from the New York Daily News, gave it an 80 grade, saying: \\u201cCraig is cruelly efficient. Dave Bautista makes a good, Oddjob-like assassin. And while Lea Seydoux doesn\\u2019t leave a huge impression as this film\\u2019s \\u201cBond girl,\\u201d perhaps it\\u2019s because we\\u2019ve already met \\u2014 far too briefly \\u2014 the hypnotic Monica Bellucci, as the first real \\u201cBond woman\\u201d since Diana Rigg.\\u201d Richard Roeper from the Chicago Sun-Times, gave it a 75 grade. He stated: \\u201cThis is the 24th Bond film and it ranks solidly in the middle of the all-time rankings, which means it\\u2019s still a slick, beautifully photographed, action-packed, international thriller with a number of wonderfully, ludicrously entertaining set pieces, a sprinkling of dry wit, myriad gorgeous women and a classic psycho-villain who is clearly out of his mind but seems to like it that way.\\u201d Michael Phillips over at the Chicago Tribune, gave it a 75 grade. He stated: \\u201cFor all its workmanlike devotion to out-of-control helicopters, \\u201cSpectre\\u201d works best when everyone\\u2019s on the ground, doing his or her job, driving expensive fast cars heedlessly, detonating the occasional wisecrack, enjoying themselves and their beautiful clothes.\\u201d Guy Lodge from Variety, gave it a 70 score, stating: \\u201cWhat\\u2019s missing is the unexpected emotional urgency of \\u201cSkyfall,\\u201d as the film sustains its predecessor\\u2019s nostalgia kick with a less sentimental bent.\\u201d\"\"\"\n",
    "net.prediction(question,context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fc9b2391-b968-47b9-aaf8-341de851fc42",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save({'state_dict': net.state_dict(), 'epoch':3},'xlnetCluster2'+str(3) + '.pkl')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
