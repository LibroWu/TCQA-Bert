{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7870213d-452e-4761-bf41-353d3e95c981",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: http://mirrors.aliyun.com/pypi/simple\n",
      "Collecting transformers\n",
      "  Downloading http://mirrors.aliyun.com/pypi/packages/70/10/4f0924b0301042f226ed07c3273d7ae4577744033ef1309e770da8f7e03e/transformers-4.19.1-py3-none-any.whl (4.2 MB)\n",
      "\u001b[K     |████████████████████████████████| 4.2 MB 8.9 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting huggingface-hub<1.0,>=0.1.0\n",
      "  Downloading http://mirrors.aliyun.com/pypi/packages/c1/f2/d6542f1e29b803442e058f7a1b52313bea37da46517b1e840ff2f166450c/huggingface_hub-0.6.0-py3-none-any.whl (84 kB)\n",
      "\u001b[K     |████████████████████████████████| 84 kB 11.2 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting filelock\n",
      "  Downloading http://mirrors.aliyun.com/pypi/packages/e8/74/48523f5206b0930f7c6b312890c7ab285dba55cea3f0a303999c5425df08/filelock-3.7.0-py3-none-any.whl (10 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in ./miniconda3/lib/python3.8/site-packages (from transformers) (1.21.4)\n",
      "Requirement already satisfied: requests in ./miniconda3/lib/python3.8/site-packages (from transformers) (2.25.1)\n",
      "Collecting tokenizers!=0.11.3,<0.13,>=0.11.1\n",
      "  Downloading http://mirrors.aliyun.com/pypi/packages/36/fa/e22ebbcaeecd9bd04efa30f7ec43ccf1501c97615c9af3bbf13a77ce0b81/tokenizers-0.12.1-cp38-cp38-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.6 MB)\n",
      "\u001b[K     |████████████████████████████████| 6.6 MB 7.4 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in ./miniconda3/lib/python3.8/site-packages (from transformers) (4.61.2)\n",
      "Collecting regex!=2019.12.17\n",
      "  Downloading http://mirrors.aliyun.com/pypi/packages/d8/e8/4331ed657cbea0fcc16bc57320d3a34e734d6a49dff1760beeedf827d26d/regex-2022.4.24-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (764 kB)\n",
      "\u001b[K     |████████████████████████████████| 764 kB 9.2 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: packaging>=20.0 in ./miniconda3/lib/python3.8/site-packages (from transformers) (21.3)\n",
      "Collecting pyyaml>=5.1\n",
      "  Downloading http://mirrors.aliyun.com/pypi/packages/d7/42/7ad4b6d67a16229496d4f6e74201bdbebcf4bc1e87d5a70c9297d4961bd2/PyYAML-6.0-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (701 kB)\n",
      "\u001b[K     |████████████████████████████████| 701 kB 8.3 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: typing-extensions>=3.7.4.3 in ./miniconda3/lib/python3.8/site-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (4.0.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in ./miniconda3/lib/python3.8/site-packages (from packaging>=20.0->transformers) (3.0.6)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./miniconda3/lib/python3.8/site-packages (from requests->transformers) (2021.5.30)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in ./miniconda3/lib/python3.8/site-packages (from requests->transformers) (1.26.6)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in ./miniconda3/lib/python3.8/site-packages (from requests->transformers) (4.0.0)\n",
      "Requirement already satisfied: idna<3,>=2.5 in ./miniconda3/lib/python3.8/site-packages (from requests->transformers) (2.10)\n",
      "Installing collected packages: pyyaml, filelock, tokenizers, regex, huggingface-hub, transformers\n",
      "Successfully installed filelock-3.7.0 huggingface-hub-0.6.0 pyyaml-6.0 regex-2022.4.24 tokenizers-0.12.1 transformers-4.19.1\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "Looking in indexes: http://mirrors.aliyun.com/pypi/simple\n",
      "Collecting tensorflow\n",
      "  Downloading http://mirrors.aliyun.com/pypi/packages/2f/45/f5c91d69c2121e8e60673164bdcd2c6cda7b89e37decbc3c01b0466ca990/tensorflow-2.8.0-cp38-cp38-manylinux2010_x86_64.whl (497.6 MB)\n",
      "\u001b[K     |████████████████████████████████| 497.6 MB 6.6 MB/s eta 0:00:01     |███████▍                        | 114.6 MB 6.6 MB/s eta 0:00:58     |███████████████                 | 232.1 MB 8.0 MB/s eta 0:00:34     |████████████████▎               | 253.6 MB 8.1 MB/s eta 0:00:31     |█████████████████▎              | 268.3 MB 6.4 MB/s eta 0:00:36     |██████████████████████████      | 403.9 MB 8.7 MB/s eta 0:00:11\n",
      "\u001b[?25hCollecting h5py>=2.9.0\n",
      "  Downloading http://mirrors.aliyun.com/pypi/packages/d4/d8/6a121064f0e357dc24e5ab53945e9ec057bb1e5ca7da60355d06c89a3d36/h5py-3.6.0-cp38-cp38-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (4.5 MB)\n",
      "\u001b[K     |████████████████████████████████| 4.5 MB 8.4 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting libclang>=9.0.1\n",
      "  Downloading http://mirrors.aliyun.com/pypi/packages/ab/2f/c6f380aec0b064bccbd81141fecba9862b5634c838f13fff727adc84ceb9/libclang-14.0.1-py2.py3-none-manylinux1_x86_64.whl (14.5 MB)\n",
      "\u001b[K     |████████████████████████████████| 14.5 MB 6.5 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: protobuf>=3.9.2 in ./miniconda3/lib/python3.8/site-packages (from tensorflow) (3.19.1)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in ./miniconda3/lib/python3.8/site-packages (from tensorflow) (1.42.0)\n",
      "Collecting keras-preprocessing>=1.1.1\n",
      "  Downloading http://mirrors.aliyun.com/pypi/packages/79/4c/7c3275a01e12ef9368a892926ab932b33bb13d55794881e3573482b378a7/Keras_Preprocessing-1.1.2-py2.py3-none-any.whl (42 kB)\n",
      "\u001b[K     |████████████████████████████████| 42 kB 12.7 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting tensorflow-io-gcs-filesystem>=0.23.1\n",
      "  Downloading http://mirrors.aliyun.com/pypi/packages/ba/7c/13c8a2e1cf7e2f4a898c75ff388a1873aefb8e3037368030545576bd67ee/tensorflow_io_gcs_filesystem-0.25.0-cp38-cp38-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (2.1 MB)\n",
      "\u001b[K     |████████████████████████████████| 2.1 MB 13.6 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: six>=1.12.0 in ./miniconda3/lib/python3.8/site-packages (from tensorflow) (1.16.0)\n",
      "Collecting google-pasta>=0.1.1\n",
      "  Downloading http://mirrors.aliyun.com/pypi/packages/a3/de/c648ef6835192e6e2cc03f40b19eeda4382c49b5bafb43d88b931c4c74ac/google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "\u001b[K     |████████████████████████████████| 57 kB 17.2 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: typing-extensions>=3.6.6 in ./miniconda3/lib/python3.8/site-packages (from tensorflow) (4.0.0)\n",
      "Requirement already satisfied: absl-py>=0.4.0 in ./miniconda3/lib/python3.8/site-packages (from tensorflow) (1.0.0)\n",
      "Collecting keras<2.9,>=2.8.0rc0\n",
      "  Downloading http://mirrors.aliyun.com/pypi/packages/4f/2f/eb9391bdcba2693cc8396f244bd3b4512bcd1123c2ea06f4dfcf50dc5ce9/keras-2.8.0-py2.py3-none-any.whl (1.4 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.4 MB 12.9 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting termcolor>=1.1.0\n",
      "  Downloading http://mirrors.aliyun.com/pypi/packages/8a/48/a76be51647d0eb9f10e2a4511bf3ffb8cc1e6b14e9e4fab46173aa79f981/termcolor-1.1.0.tar.gz (3.9 kB)\n",
      "Collecting wrapt>=1.11.0\n",
      "  Downloading http://mirrors.aliyun.com/pypi/packages/36/ee/944dc7e5462662270e8a379755bcc543fc8f09029866288060dc163ed5b4/wrapt-1.14.1-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (81 kB)\n",
      "\u001b[K     |████████████████████████████████| 81 kB 15.5 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: setuptools in ./miniconda3/lib/python3.8/site-packages (from tensorflow) (52.0.0.post20210125)\n",
      "Collecting tensorboard<2.9,>=2.8\n",
      "  Downloading http://mirrors.aliyun.com/pypi/packages/f7/fd/67c61276de025801cfa8a1b9af2d7c577e7f27c17b6bff2baca20bf03543/tensorboard-2.8.0-py3-none-any.whl (5.8 MB)\n",
      "\u001b[K     |████████████████████████████████| 5.8 MB 11.5 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting astunparse>=1.6.0\n",
      "  Downloading http://mirrors.aliyun.com/pypi/packages/2b/03/13dde6512ad7b4557eb792fbcf0c653af6076b81e5941d36ec61f7ce6028/astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Requirement already satisfied: numpy>=1.20 in ./miniconda3/lib/python3.8/site-packages (from tensorflow) (1.21.4)\n",
      "Collecting opt-einsum>=2.3.2\n",
      "  Downloading http://mirrors.aliyun.com/pypi/packages/bc/19/404708a7e54ad2798907210462fd950c3442ea51acc8790f3da48d2bee8b/opt_einsum-3.3.0-py3-none-any.whl (65 kB)\n",
      "\u001b[K     |████████████████████████████████| 65 kB 7.9 MB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting flatbuffers>=1.12\n",
      "  Downloading http://mirrors.aliyun.com/pypi/packages/3d/d0/26033c70d642fbc1e35d3619cf3210986fb953c173b1226709f75056c149/flatbuffers-2.0-py2.py3-none-any.whl (26 kB)\n",
      "Collecting gast>=0.2.1\n",
      "  Downloading http://mirrors.aliyun.com/pypi/packages/5f/1c/b59500a88c5c3d9d601c5ca62b9df5e0964764472faed82a182958a922c5/gast-0.5.3-py3-none-any.whl (19 kB)\n",
      "Collecting tf-estimator-nightly==2.8.0.dev2021122109\n",
      "  Downloading http://mirrors.aliyun.com/pypi/packages/a7/f1/f89e097f377b163856076f167baf149b010df3bbf425d2c06276048e2051/tf_estimator_nightly-2.8.0.dev2021122109-py2.py3-none-any.whl (462 kB)\n",
      "\u001b[K     |████████████████████████████████| 462 kB 12.9 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: wheel<1.0,>=0.23.0 in ./miniconda3/lib/python3.8/site-packages (from astunparse>=1.6.0->tensorflow) (0.36.2)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in ./miniconda3/lib/python3.8/site-packages (from tensorboard<2.9,>=2.8->tensorflow) (0.4.6)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in ./miniconda3/lib/python3.8/site-packages (from tensorboard<2.9,>=2.8->tensorflow) (2.0.2)\n",
      "Requirement already satisfied: markdown>=2.6.8 in ./miniconda3/lib/python3.8/site-packages (from tensorboard<2.9,>=2.8->tensorflow) (3.3.6)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in ./miniconda3/lib/python3.8/site-packages (from tensorboard<2.9,>=2.8->tensorflow) (1.8.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in ./miniconda3/lib/python3.8/site-packages (from tensorboard<2.9,>=2.8->tensorflow) (0.6.1)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in ./miniconda3/lib/python3.8/site-packages (from tensorboard<2.9,>=2.8->tensorflow) (2.25.1)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in ./miniconda3/lib/python3.8/site-packages (from tensorboard<2.9,>=2.8->tensorflow) (2.3.3)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in ./miniconda3/lib/python3.8/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow) (4.8)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in ./miniconda3/lib/python3.8/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow) (4.2.4)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in ./miniconda3/lib/python3.8/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow) (0.2.8)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in ./miniconda3/lib/python3.8/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow) (1.3.0)\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in ./miniconda3/lib/python3.8/site-packages (from markdown>=2.6.8->tensorboard<2.9,>=2.8->tensorflow) (4.8.2)\n",
      "Requirement already satisfied: zipp>=0.5 in ./miniconda3/lib/python3.8/site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.9,>=2.8->tensorflow) (3.6.0)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in ./miniconda3/lib/python3.8/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow) (0.4.8)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in ./miniconda3/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow) (4.0.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./miniconda3/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow) (2021.5.30)\n",
      "Requirement already satisfied: idna<3,>=2.5 in ./miniconda3/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow) (2.10)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in ./miniconda3/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow) (1.26.6)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in ./miniconda3/lib/python3.8/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow) (3.1.1)\n",
      "Building wheels for collected packages: termcolor\n",
      "  Building wheel for termcolor (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for termcolor: filename=termcolor-1.1.0-py3-none-any.whl size=4829 sha256=8a212fa91428427f2bfec1561fb0479a39cf9f08ef559036e0afef9ca8c465db\n",
      "  Stored in directory: /root/.cache/pip/wheels/77/83/72/2f8fd2274618dabac14ac44c425333b4d9cd6cb296243af7f1\n",
      "Successfully built termcolor\n",
      "Installing collected packages: wrapt, tf-estimator-nightly, termcolor, tensorflow-io-gcs-filesystem, tensorboard, opt-einsum, libclang, keras-preprocessing, keras, h5py, google-pasta, gast, flatbuffers, astunparse, tensorflow\n",
      "  Attempting uninstall: tensorboard\n",
      "    Found existing installation: tensorboard 2.7.0\n",
      "    Uninstalling tensorboard-2.7.0:\n",
      "      Successfully uninstalled tensorboard-2.7.0\n",
      "Successfully installed astunparse-1.6.3 flatbuffers-2.0 gast-0.5.3 google-pasta-0.2.0 h5py-3.6.0 keras-2.8.0 keras-preprocessing-1.1.2 libclang-14.0.1 opt-einsum-3.3.0 tensorboard-2.8.0 tensorflow-2.8.0 tensorflow-io-gcs-filesystem-0.25.0 termcolor-1.1.0 tf-estimator-nightly-2.8.0.dev2021122109 wrapt-1.14.1\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers\n",
    "!pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0efd1339-aca4-4eef-839c-3f8c7dbb7049",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'|===========================================================================|\\n|                  PyTorch CUDA memory summary, device ID 0                 |\\n|---------------------------------------------------------------------------|\\n|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |\\n|===========================================================================|\\n|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\\n|---------------------------------------------------------------------------|\\n| Allocated memory      |       0 B  |       0 B  |       0 B  |       0 B  |\\n|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |\\n|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |\\n|---------------------------------------------------------------------------|\\n| Active memory         |       0 B  |       0 B  |       0 B  |       0 B  |\\n|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |\\n|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |\\n|---------------------------------------------------------------------------|\\n| GPU reserved memory   |       0 B  |       0 B  |       0 B  |       0 B  |\\n|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |\\n|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |\\n|---------------------------------------------------------------------------|\\n| Non-releasable memory |       0 B  |       0 B  |       0 B  |       0 B  |\\n|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |\\n|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |\\n|---------------------------------------------------------------------------|\\n| Allocations           |       0    |       0    |       0    |       0    |\\n|       from large pool |       0    |       0    |       0    |       0    |\\n|       from small pool |       0    |       0    |       0    |       0    |\\n|---------------------------------------------------------------------------|\\n| Active allocs         |       0    |       0    |       0    |       0    |\\n|       from large pool |       0    |       0    |       0    |       0    |\\n|       from small pool |       0    |       0    |       0    |       0    |\\n|---------------------------------------------------------------------------|\\n| GPU reserved segments |       0    |       0    |       0    |       0    |\\n|       from large pool |       0    |       0    |       0    |       0    |\\n|       from small pool |       0    |       0    |       0    |       0    |\\n|---------------------------------------------------------------------------|\\n| Non-releasable allocs |       0    |       0    |       0    |       0    |\\n|       from large pool |       0    |       0    |       0    |       0    |\\n|       from small pool |       0    |       0    |       0    |       0    |\\n|---------------------------------------------------------------------------|\\n| Oversize allocations  |       0    |       0    |       0    |       0    |\\n|---------------------------------------------------------------------------|\\n| Oversize GPU segments |       0    |       0    |       0    |       0    |\\n|===========================================================================|\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.cuda.empty_cache()\n",
    "torch.cuda.memory_summary(device=None, abbreviated=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "309ef23a-18bb-43f4-84dc-a920963d524b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import csv\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import TensorDataset\n",
    "from PIL import Image, ImageFile\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision import models\n",
    "import torch.utils.model_zoo as model_zoo\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import os\n",
    "import json\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import operator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5df42a46-aeb2-4992-847d-cea7b6c96f59",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda')\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "pre_trained_model = BertModel.from_pretrained(\"bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e6b78e08-8424-40fc-8338-de7618b480fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContextQuType:\n",
    "    def __init__(self, tokenizer, length, ids, context, question):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.codeLength = length\n",
    "        self.ids = ids\n",
    "        self.context = context\n",
    "        self.question = question\n",
    "        self.after_encode = 0\n",
    "        \n",
    "    def convert(self):\n",
    "        if self.after_encode==0:\n",
    "            after_encode = tokenizer(self.question,self.context,max_length=self.codeLength,padding=\"max_length\")\n",
    "        return torch.stack([torch.tensor(after_encode['input_ids'][:512]),torch.tensor(after_encode['token_type_ids'][:512]),torch.tensor(after_encode['attention_mask'][:512])])\n",
    "            \n",
    "class LabelType:\n",
    "    def __init__(self, tokenizer, length, CQu, has_ans, text, start):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.codeLength = length\n",
    "        self.has_ans = has_ans\n",
    "        self.text = text.lower()\n",
    "        self.start = start\n",
    "        self.CQu = CQu\n",
    "    \n",
    "    def convert(self):\n",
    "        # make groundtruth\n",
    "        S = [0.0] * self.codeLength\n",
    "        E = [0.0] * self.codeLength\n",
    "        background = tokenizer.encode(self.CQu.question,self.CQu.context,max_length=self.codeLength,padding=\"max_length\")\n",
    "        if self.has_ans:\n",
    "            after_encode = tokenizer.encode(self.text)[1:-1]\n",
    "            ans_length = len(after_encode)\n",
    "            before_answer = tokenizer.encode(self.CQu.question,self.CQu.context[:self.start])\n",
    "            start_id = len(before_answer) - 1\n",
    "            end_id = start_id + ans_length - 1\n",
    "            if tokenizer.decode(background[start_id:end_id+1])!=self.text:\n",
    "                    lower = max(start_id-3,0)\n",
    "                    upper = min(end_id+4,len(background))\n",
    "                    ans_found = False\n",
    "                    for i in range(lower,upper):\n",
    "                        for j in range(i+1,upper):\n",
    "                            candidate = tokenizer.decode(background[i:j])\n",
    "                            #if len(candidate)>0:\n",
    "                            #    if candidate[0]=='$' and self.text[0]!='$':\n",
    "                            #        candidate=candidate[1:]\n",
    "                            #    if candidate[0]!='$' and self.text[0]=='$':\n",
    "                            #        candidate='$'+candidate\n",
    "                            if candidate==self.text:\n",
    "                                start_id = i\n",
    "                                end_id = j-1\n",
    "                                ans_found = True\n",
    "                                break\n",
    "                            if ans_found:\n",
    "                                break\n",
    "            if start_id >= self.codeLength:\n",
    "                start_id = 0\n",
    "                end_id = 0\n",
    "            end_id = min(end_id,self.codeLength - 1)\n",
    "            S[start_id] = 1.0\n",
    "            E[end_id] = 1.0\n",
    "        else:\n",
    "            S[0] = 1.0\n",
    "            E[0] = 1.0\n",
    "        return torch.stack([torch.tensor(S),torch.tensor(E)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5b96639f-9c3d-4c7b-9b74-fde3927766f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(load_path,tokenizer,length):\n",
    "    with open(load_path) as json_data:\n",
    "        dev = json.load(json_data)['data']\n",
    "    CQu = []\n",
    "    Label = []\n",
    "    for data in dev:\n",
    "        for paragraphs in data['paragraphs']:\n",
    "            context = paragraphs['context']\n",
    "            for qas in paragraphs['qas']:\n",
    "                question = qas['question']\n",
    "                ids = qas['id']\n",
    "                ctx = ContextQuType(tokenizer,length,ids,context,question)\n",
    "                CQu.append(ctx)\n",
    "                if qas['is_impossible']:\n",
    "                    Label.append(LabelType(tokenizer,length,ctx,False,'',0))\n",
    "                else:\n",
    "                    answer = qas['answers'][0]\n",
    "                    Label.append(LabelType(tokenizer,length,ctx,True,answer['text'],answer['answer_start']))\n",
    "    return CQu,Label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7dc85e75-9fa2-4082-bd42-fef5da59b14c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OutputPair:\n",
    "    def __init__(self,logits_start,logits_end):\n",
    "        self.logits_start = logits_start\n",
    "        self.logits_end = logits_end\n",
    "\n",
    "class CrossEntropy(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CrossEntropy, self).__init__()\n",
    "    def forward(self,outputs,labels):\n",
    "        log_soft = F.log_softmax(outputs.logits_start,dim=1)\n",
    "        loss_start = F.cross_entropy(log_soft, labels[:,0])\n",
    "        loss_end   = F.cross_entropy(F.log_softmax(outputs.logits_end,dim=1), labels[:,1])\n",
    "        return loss_start + loss_end / 2.0\n",
    "\n",
    "class BERTQuA(nn.Module):\n",
    "    def __init__(self, bert, tokenizer,code_length,device,epsilon = 1.0):\n",
    "        super(BERTQuA, self).__init__()\n",
    "        self.tokenizer = tokenizer\n",
    "        self.device = device\n",
    "        self.encoder = bert.to(device)\n",
    "        self.code_length = code_length\n",
    "        self.output_start = nn.Linear(768,1).to(device)\n",
    "        self.output_end = nn.Linear(768,1).to(device)\n",
    "        self.epsilon = 1.0\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        tokens_X, segments_X, masks = inputs[:,0],inputs[:,1],inputs[:,2]\n",
    "        encoded_X = self.encoder(tokens_X, token_type_ids=segments_X, attention_mask=masks).last_hidden_state\n",
    "        return OutputPair(self.output_start(encoded_X),self.output_end(encoded_X))\n",
    "        \n",
    "    def prediction(self,question,answer_text):\n",
    "        ctx = ContextQuType(self.tokenizer,self.code_length,'',answer_text,question)\n",
    "        inputs = torch.stack([ctx.convert()]).to(device)\n",
    "        token_ids = tokenizer(question,answer_text,max_length=self.code_length,padding=\"max_length\")['input_ids']\n",
    "        outputs = self.forward(inputs)\n",
    "        score_len = outputs.logits_start.shape[1]\n",
    "        start_scores = outputs.logits_start.reshape(score_len)\n",
    "        end_scores = outputs.logits_end.reshape(score_len)\n",
    "        no_ans_scores = start_scores[0] + end_scores[0]\n",
    "        answer = ''\n",
    "        flag = False\n",
    "        tokens = tokenizer.convert_ids_to_tokens(token_ids)\n",
    "        if (torch.max(start_scores) + torch.max(end_scores) <= no_ans_scores +  self.epsilon):\n",
    "            answer = ''\n",
    "        else:\n",
    "            answer_start = torch.argmax(start_scores)\n",
    "            answer_end = torch.argmax(end_scores)\n",
    "            answer = self.tokenizer.decode(token_ids[answer_start:answer_end+1])\n",
    "        return answer\n",
    "    \n",
    "    def predictionLabel(self,label):\n",
    "        #ctx = ContextQuType(self.tokenizer,self.code_length,'',answer_text,question)\n",
    "        #inputs = torch.stack([ctx.convert()]).to(device)\n",
    "        #token_ids = tokenizer(question,answer_text,max_length=self.code_length,padding=\"max_length\")['input_ids']\n",
    "        #outputs = self.forward(inputs)\n",
    "        outputs = label.convert()\n",
    "        #print(outputs)\n",
    "        token_ids = tokenizer(label.CQu.question,label.CQu.context,max_length=self.code_length,padding=\"max_length\")['input_ids']\n",
    "        start_scores = outputs[0,:]\n",
    "        end_scores = outputs[1,:]\n",
    "        #print(start_scores,end_scores)\n",
    "        no_ans_scores = start_scores[0] + end_scores[0]\n",
    "        answer = ''\n",
    "        tokens = tokenizer.convert_ids_to_tokens(token_ids)\n",
    "        #print(outputs.logits_start.reshape(-1))\n",
    "        #print(outputs.logits_end.reshape(-1))\n",
    "        if (torch.max(start_scores) + torch.max(end_scores) <= no_ans_scores +  self.epsilon):\n",
    "            answer = ''\n",
    "        else:\n",
    "            answer_start = torch.argmax(start_scores)\n",
    "            answer_end = torch.argmax(end_scores)\n",
    "            answer = self.tokenizer.decode(token_ids[answer_start:answer_end+1])\n",
    "        return answer\n",
    "    def load_data(self,file_path):\n",
    "        CQu, La = load_data(file_path,self.tokenizer,self.code_length)\n",
    "        self.inputs = torch.stack([ctx.convert() for ctx in CQu])\n",
    "        self.labels = torch.stack([ctx.convert() for ctx in La])\n",
    "    \n",
    "    def train(self, epochs=3, batch_size = 48, lr = 3e-5, T=10):\n",
    "        optimizer = optim.Adam(self.parameters(), lr=lr)\n",
    "        criterion = CrossEntropy().to(device)\n",
    "        self.train_loader = DataLoader(TensorDataset(self.inputs,self.labels),batch_size = batch_size, shuffle = True, num_workers = 2)\n",
    "        epoch_loss = []\n",
    "        for epoch in range(epochs):      \n",
    "            running_loss = 0.0\n",
    "            epoch_running_loss = 0.0\n",
    "            batch_count = 0\n",
    "            for batchidx, (x, label) in enumerate(self.train_loader):\n",
    "                x, label = x.to(device), label.to(device)\n",
    "                output = self.forward(x)\n",
    "                output.logits_start=output.logits_start.resize(len(output.logits_start),self.code_length)\n",
    "                output.logits_end=output.logits_end.resize(len(output.logits_end),self.code_length)\n",
    "                loss = criterion(output, label)\n",
    "                # backprop\n",
    "                optimizer.zero_grad()  #梯度清0\n",
    "                loss.backward()   #梯度反传\n",
    "                optimizer.step()   #保留梯度\n",
    "                \n",
    "                running_loss += loss.item()\n",
    "                epoch_running_loss += loss.item()\n",
    "                batch_count += 1\n",
    "                if batchidx % T == T-1:\n",
    "                    print(epoch,' batchidx: ', batchidx, ' loss: ', running_loss/T)\n",
    "                    running_loss = 0.0\n",
    "            epoch_loss.append(epoch_running_loss/batch_count)\n",
    "            print(epoch, 'loss:', epoch_running_loss/batch_count)\n",
    "        return epoch_loss\n",
    "    def printLabel(self,file_path):\n",
    "        CQu, La = load_data(file_path,self.tokenizer,self.code_length)\n",
    "        count = 0\n",
    "        precise = 0\n",
    "        for label in La:\n",
    "            count += 1\n",
    "            predict = self.predictionLabel(label)\n",
    "            if predict==label.text:\n",
    "                precise += 1\n",
    "            else:\n",
    "                print(predict,label.text,label.has_ans)\n",
    "        print(precise/count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "50ec9d62-0604-4eed-83c3-8b6e1aeaa123",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda')\n",
    "net = BERTQuA(pre_trained_model,tokenizer,512,device)\n",
    "net.load_data('train-v2.0-Tag1.0.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "48ba42be-008a-4086-a94b-9175f2f4c8b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/lib/python3.8/site-packages/torch/_tensor.py:493: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0  batchidx:  9  loss:  8.375053596496581\n",
      "0  batchidx:  19  loss:  6.674959278106689\n",
      "0  batchidx:  29  loss:  5.4409748077392575\n",
      "0  batchidx:  39  loss:  5.341647148132324\n",
      "0  batchidx:  49  loss:  4.824629902839661\n",
      "0  batchidx:  59  loss:  4.046962070465088\n",
      "0  batchidx:  69  loss:  4.170640397071838\n",
      "0  batchidx:  79  loss:  3.8895325660705566\n",
      "0  batchidx:  89  loss:  3.5661971807479858\n",
      "0  batchidx:  99  loss:  3.508423352241516\n",
      "0  batchidx:  109  loss:  3.3395523548126222\n",
      "0  batchidx:  119  loss:  3.0631514310836794\n",
      "0  batchidx:  129  loss:  3.2824429750442503\n",
      "0  batchidx:  139  loss:  3.0704012870788575\n",
      "0  batchidx:  149  loss:  2.967134141921997\n",
      "0  batchidx:  159  loss:  3.3613591313362123\n",
      "0  batchidx:  169  loss:  3.0499977588653566\n",
      "0  batchidx:  179  loss:  2.4612540125846865\n",
      "0  batchidx:  189  loss:  2.8389235615730284\n",
      "0  batchidx:  199  loss:  2.5803094625473024\n",
      "0  batchidx:  209  loss:  2.9079527258872986\n",
      "0  batchidx:  219  loss:  2.6807554841041563\n",
      "0  batchidx:  229  loss:  2.3526760816574095\n",
      "0  batchidx:  239  loss:  2.7392730951309203\n",
      "0  batchidx:  249  loss:  2.856698739528656\n",
      "0  batchidx:  259  loss:  2.5540610551834106\n",
      "0  batchidx:  269  loss:  2.392025327682495\n",
      "0  batchidx:  279  loss:  2.565892791748047\n",
      "0  batchidx:  289  loss:  2.3797644138336183\n",
      "0  batchidx:  299  loss:  2.2552411198616027\n",
      "0  batchidx:  309  loss:  2.3651535272598267\n",
      "0  batchidx:  319  loss:  2.416075015068054\n",
      "0  batchidx:  329  loss:  2.558526933193207\n",
      "0  batchidx:  339  loss:  2.5585357189178466\n",
      "0  batchidx:  349  loss:  2.4983380317687987\n",
      "0  batchidx:  359  loss:  2.327941966056824\n",
      "0  batchidx:  369  loss:  2.5199276208877563\n",
      "0  batchidx:  379  loss:  2.286081039905548\n",
      "0  batchidx:  389  loss:  2.482273256778717\n",
      "0  batchidx:  399  loss:  2.5187558770179748\n",
      "0  batchidx:  409  loss:  2.0857030034065245\n",
      "0  batchidx:  419  loss:  2.4770612716674805\n",
      "0  batchidx:  429  loss:  2.2291144371032714\n",
      "0  batchidx:  439  loss:  2.396100699901581\n",
      "0  batchidx:  449  loss:  2.4877366542816164\n",
      "0  batchidx:  459  loss:  2.2414921522140503\n",
      "0  batchidx:  469  loss:  2.5536847710609436\n",
      "0  batchidx:  479  loss:  2.485539960861206\n",
      "0  batchidx:  489  loss:  2.33666170835495\n",
      "0  batchidx:  499  loss:  2.27544823884964\n",
      "0  batchidx:  509  loss:  2.151101303100586\n",
      "0  batchidx:  519  loss:  2.387903606891632\n",
      "0  batchidx:  529  loss:  2.314091455936432\n",
      "0  batchidx:  539  loss:  2.139953303337097\n",
      "0  batchidx:  549  loss:  2.009971487522125\n",
      "0  batchidx:  559  loss:  2.3915260314941404\n",
      "0  batchidx:  569  loss:  2.4288224458694456\n",
      "0  batchidx:  579  loss:  2.3545275568962096\n",
      "0  batchidx:  589  loss:  2.205938792228699\n",
      "0  batchidx:  599  loss:  2.4462416887283327\n",
      "0  batchidx:  609  loss:  2.1761566162109376\n",
      "0  batchidx:  619  loss:  2.4443442583084107\n",
      "0  batchidx:  629  loss:  2.3257980585098266\n",
      "0  batchidx:  639  loss:  2.1680935978889466\n",
      "0  batchidx:  649  loss:  2.378125250339508\n",
      "0  batchidx:  659  loss:  2.1692138433456423\n",
      "0  batchidx:  669  loss:  2.3775704741477965\n",
      "0  batchidx:  679  loss:  2.1207282304763795\n",
      "0  batchidx:  689  loss:  2.324064338207245\n",
      "0  batchidx:  699  loss:  2.4663195610046387\n",
      "0  batchidx:  709  loss:  2.161406767368317\n",
      "0  batchidx:  719  loss:  2.2985820889472963\n",
      "0  batchidx:  729  loss:  2.1652673840522767\n",
      "0  batchidx:  739  loss:  2.51195707321167\n",
      "0  batchidx:  749  loss:  2.0167673230171204\n",
      "0  batchidx:  759  loss:  2.1322396874427794\n",
      "0  batchidx:  769  loss:  2.1928056478500366\n",
      "0  batchidx:  779  loss:  2.4510850429534914\n",
      "0  batchidx:  789  loss:  2.359126424789429\n",
      "0  batchidx:  799  loss:  1.9886078476905822\n",
      "0  batchidx:  809  loss:  2.033741557598114\n",
      "0  batchidx:  819  loss:  2.1445364594459533\n",
      "0  batchidx:  829  loss:  2.1650826215744017\n",
      "0  batchidx:  839  loss:  2.192304801940918\n",
      "0  batchidx:  849  loss:  2.1663870334625246\n",
      "0  batchidx:  859  loss:  1.9844985485076905\n",
      "0  batchidx:  869  loss:  2.308088707923889\n",
      "0  batchidx:  879  loss:  2.428342032432556\n",
      "0  batchidx:  889  loss:  1.6978379011154174\n",
      "0  batchidx:  899  loss:  1.9405967831611632\n",
      "0  batchidx:  909  loss:  2.079470992088318\n",
      "0  batchidx:  919  loss:  1.97677161693573\n",
      "0  batchidx:  929  loss:  1.9150753498077393\n",
      "0  batchidx:  939  loss:  1.9570573687553405\n",
      "0  batchidx:  949  loss:  2.1736913919448853\n",
      "0  batchidx:  959  loss:  1.8093223094940185\n",
      "0  batchidx:  969  loss:  1.9522870302200317\n",
      "0  batchidx:  979  loss:  2.0380744457244875\n",
      "0  batchidx:  989  loss:  1.93237247467041\n",
      "0  batchidx:  999  loss:  2.0068303465843202\n",
      "0  batchidx:  1009  loss:  2.0186954021453856\n",
      "0  batchidx:  1019  loss:  1.8397537469863892\n",
      "0  batchidx:  1029  loss:  2.1237355351448057\n",
      "0  batchidx:  1039  loss:  2.050817000865936\n",
      "0  batchidx:  1049  loss:  2.1906896829605103\n",
      "0  batchidx:  1059  loss:  2.0927402138710023\n",
      "0  batchidx:  1069  loss:  2.4203227043151854\n",
      "0  batchidx:  1079  loss:  1.8892682790756226\n",
      "0  batchidx:  1089  loss:  1.965172004699707\n",
      "0  batchidx:  1099  loss:  2.0311999917030334\n",
      "0  batchidx:  1109  loss:  2.049920582771301\n",
      "0  batchidx:  1119  loss:  2.025211274623871\n",
      "0  batchidx:  1129  loss:  2.173644649982452\n",
      "0  batchidx:  1139  loss:  1.9748140811920165\n",
      "0  batchidx:  1149  loss:  1.7164936065673828\n",
      "0  batchidx:  1159  loss:  1.9833763003349305\n",
      "0  batchidx:  1169  loss:  2.3063111305236816\n",
      "0  batchidx:  1179  loss:  1.9827470421791076\n",
      "0  batchidx:  1189  loss:  2.099426007270813\n",
      "0  batchidx:  1199  loss:  1.8857695817947389\n",
      "0  batchidx:  1209  loss:  2.236929953098297\n",
      "0  batchidx:  1219  loss:  2.0013789772987365\n",
      "0  batchidx:  1229  loss:  1.9180004477500916\n",
      "0  batchidx:  1239  loss:  1.8193976879119873\n",
      "0  batchidx:  1249  loss:  1.970112383365631\n",
      "0  batchidx:  1259  loss:  1.8723538517951965\n",
      "0  batchidx:  1269  loss:  1.87463219165802\n",
      "0  batchidx:  1279  loss:  2.178022396564484\n",
      "0  batchidx:  1289  loss:  1.945047640800476\n",
      "0  batchidx:  1299  loss:  2.0434441328048707\n",
      "0  batchidx:  1309  loss:  1.9090272068977356\n",
      "0  batchidx:  1319  loss:  1.767906093597412\n",
      "0  batchidx:  1329  loss:  1.7834214925765992\n",
      "0  batchidx:  1339  loss:  2.0125608325004576\n",
      "0  batchidx:  1349  loss:  2.1629855036735535\n",
      "0  batchidx:  1359  loss:  1.9818440079689026\n",
      "0  batchidx:  1369  loss:  1.6720253586769105\n",
      "0  batchidx:  1379  loss:  2.0780191659927367\n",
      "0  batchidx:  1389  loss:  1.7495898604393005\n",
      "0  batchidx:  1399  loss:  1.8308294534683227\n",
      "0  batchidx:  1409  loss:  1.937855303287506\n",
      "0  batchidx:  1419  loss:  1.8573389410972596\n",
      "0  batchidx:  1429  loss:  1.7384342432022095\n",
      "0  batchidx:  1439  loss:  2.027164173126221\n",
      "0  batchidx:  1449  loss:  2.0652392864227296\n",
      "0  batchidx:  1459  loss:  1.9575313568115233\n",
      "0  batchidx:  1469  loss:  1.7579399228096009\n",
      "0  batchidx:  1479  loss:  1.922190797328949\n",
      "0  batchidx:  1489  loss:  1.9943520426750183\n",
      "0  batchidx:  1499  loss:  2.052940344810486\n",
      "0  batchidx:  1509  loss:  1.8266112565994264\n",
      "0  batchidx:  1519  loss:  1.839024269580841\n",
      "0  batchidx:  1529  loss:  1.7568183183670043\n",
      "0  batchidx:  1539  loss:  1.7889610886573792\n",
      "0  batchidx:  1549  loss:  2.1467538475990295\n",
      "0  batchidx:  1559  loss:  2.2446867704391478\n",
      "0  batchidx:  1569  loss:  1.587578010559082\n",
      "0  batchidx:  1579  loss:  2.0032100439071656\n",
      "0  batchidx:  1589  loss:  1.7873518943786622\n",
      "0  batchidx:  1599  loss:  2.0340283274650575\n",
      "0  batchidx:  1609  loss:  1.9283777594566345\n",
      "0  batchidx:  1619  loss:  1.8579841256141663\n",
      "0  batchidx:  1629  loss:  1.9373844742774964\n",
      "0  batchidx:  1639  loss:  1.8408059358596802\n",
      "0  batchidx:  1649  loss:  2.136267697811127\n",
      "0  batchidx:  1659  loss:  2.009426462650299\n",
      "0  batchidx:  1669  loss:  1.9000585615634917\n",
      "0  batchidx:  1679  loss:  1.8169042348861695\n",
      "0  batchidx:  1689  loss:  1.8683347702026367\n",
      "0  batchidx:  1699  loss:  1.7250810146331788\n",
      "0  batchidx:  1709  loss:  2.0191121339797973\n",
      "0  batchidx:  1719  loss:  1.9509942770004272\n",
      "0  batchidx:  1729  loss:  1.6489174246788025\n",
      "0  batchidx:  1739  loss:  1.934326946735382\n",
      "0  batchidx:  1749  loss:  2.0437314033508303\n",
      "0  batchidx:  1759  loss:  1.6983725428581238\n",
      "0  batchidx:  1769  loss:  1.7941121220588685\n",
      "0  batchidx:  1779  loss:  1.5741837620735168\n",
      "0  batchidx:  1789  loss:  1.5914580345153808\n",
      "0  batchidx:  1799  loss:  1.775710690021515\n",
      "0  batchidx:  1809  loss:  1.6721632838249207\n",
      "0  batchidx:  1819  loss:  1.9030065894126893\n",
      "0  batchidx:  1829  loss:  1.764343112707138\n",
      "0  batchidx:  1839  loss:  1.8760566711425781\n",
      "0  batchidx:  1849  loss:  1.6899514675140381\n",
      "0  batchidx:  1859  loss:  1.7961106896400452\n",
      "0  batchidx:  1869  loss:  1.772104650735855\n",
      "0  batchidx:  1879  loss:  2.061954927444458\n",
      "0  batchidx:  1889  loss:  1.6172665715217591\n",
      "0  batchidx:  1899  loss:  1.9410818457603454\n",
      "0  batchidx:  1909  loss:  1.8768935799598694\n",
      "0  batchidx:  1919  loss:  1.667465615272522\n",
      "0  batchidx:  1929  loss:  1.811550462245941\n",
      "0  batchidx:  1939  loss:  1.731108295917511\n",
      "0  batchidx:  1949  loss:  1.5770686745643616\n",
      "0  batchidx:  1959  loss:  2.013774275779724\n",
      "0  batchidx:  1969  loss:  1.7954803705215454\n",
      "0  batchidx:  1979  loss:  1.6682543635368348\n",
      "0  batchidx:  1989  loss:  1.774923288822174\n",
      "0  batchidx:  1999  loss:  1.8633650541305542\n",
      "0  batchidx:  2009  loss:  1.7009355664253234\n",
      "0  batchidx:  2019  loss:  1.9930307626724244\n",
      "0  batchidx:  2029  loss:  1.535136103630066\n",
      "0 loss: 2.2586880477843008\n",
      "1  batchidx:  9  loss:  1.2445959508419038\n",
      "1  batchidx:  19  loss:  0.9301198303699494\n",
      "1  batchidx:  29  loss:  0.9895420119166374\n",
      "1  batchidx:  39  loss:  1.0132368743419646\n",
      "1  batchidx:  49  loss:  0.9410322487354279\n",
      "1  batchidx:  59  loss:  0.9718526303768158\n",
      "1  batchidx:  69  loss:  1.0758630692958833\n",
      "1  batchidx:  79  loss:  1.1859807670116425\n",
      "1  batchidx:  89  loss:  0.9230608433485031\n",
      "1  batchidx:  99  loss:  0.9757498979568482\n",
      "1  batchidx:  109  loss:  1.2492801249027252\n",
      "1  batchidx:  119  loss:  0.9466502249240876\n",
      "1  batchidx:  129  loss:  0.946415764093399\n",
      "1  batchidx:  139  loss:  1.2110081374645234\n",
      "1  batchidx:  149  loss:  1.1805191397666932\n",
      "1  batchidx:  159  loss:  1.0399581849575044\n",
      "1  batchidx:  169  loss:  1.0271629631519317\n",
      "1  batchidx:  179  loss:  1.1855148196220398\n",
      "1  batchidx:  189  loss:  1.1765399813652038\n",
      "1  batchidx:  199  loss:  1.3338590621948243\n",
      "1  batchidx:  209  loss:  1.1940469026565552\n",
      "1  batchidx:  219  loss:  0.7733066856861115\n",
      "1  batchidx:  229  loss:  1.1849874675273895\n",
      "1  batchidx:  239  loss:  1.0278076827526093\n",
      "1  batchidx:  249  loss:  0.9033982396125794\n",
      "1  batchidx:  259  loss:  0.9124702274799347\n",
      "1  batchidx:  269  loss:  0.9538885593414307\n",
      "1  batchidx:  279  loss:  0.9332935333251953\n",
      "1  batchidx:  289  loss:  0.9839161455631256\n",
      "1  batchidx:  299  loss:  0.9913655251264573\n",
      "1  batchidx:  309  loss:  0.9231977820396423\n",
      "1  batchidx:  319  loss:  1.2387468814849854\n",
      "1  batchidx:  329  loss:  1.3335627496242524\n",
      "1  batchidx:  339  loss:  1.281694084405899\n",
      "1  batchidx:  349  loss:  1.0378361821174622\n",
      "1  batchidx:  359  loss:  0.8831380009651184\n",
      "1  batchidx:  369  loss:  1.1522901713848115\n",
      "1  batchidx:  379  loss:  0.9819064378738404\n",
      "1  batchidx:  389  loss:  0.9226959884166718\n",
      "1  batchidx:  399  loss:  0.8811654865741729\n",
      "1  batchidx:  409  loss:  1.0872148752212525\n",
      "1  batchidx:  419  loss:  1.008465939760208\n",
      "1  batchidx:  429  loss:  1.1788338094949722\n",
      "1  batchidx:  439  loss:  1.0488952279090882\n",
      "1  batchidx:  449  loss:  0.9038033306598663\n",
      "1  batchidx:  459  loss:  1.038473802804947\n",
      "1  batchidx:  469  loss:  0.974412715435028\n",
      "1  batchidx:  479  loss:  0.9994504988193512\n",
      "1  batchidx:  489  loss:  1.111066609621048\n",
      "1  batchidx:  499  loss:  1.2932248145341874\n",
      "1  batchidx:  509  loss:  0.8808939814567566\n",
      "1  batchidx:  519  loss:  0.8477488040924073\n",
      "1  batchidx:  529  loss:  1.0137134671211243\n",
      "1  batchidx:  539  loss:  1.0887802362442016\n",
      "1  batchidx:  549  loss:  0.9556115210056305\n",
      "1  batchidx:  559  loss:  0.9244777798652649\n",
      "1  batchidx:  569  loss:  0.9599570810794831\n",
      "1  batchidx:  579  loss:  1.0767212688922883\n",
      "1  batchidx:  589  loss:  1.0960156440734863\n",
      "1  batchidx:  599  loss:  1.031873106956482\n",
      "1  batchidx:  609  loss:  0.8865343928337097\n",
      "1  batchidx:  619  loss:  1.0492570638656615\n",
      "1  batchidx:  629  loss:  0.9787653267383576\n",
      "1  batchidx:  639  loss:  0.8796849936246872\n",
      "1  batchidx:  649  loss:  0.8435834795236588\n",
      "1  batchidx:  659  loss:  1.2260305345058442\n",
      "1  batchidx:  669  loss:  1.1238729894161223\n",
      "1  batchidx:  679  loss:  1.0220909327268601\n",
      "1  batchidx:  689  loss:  0.9317725479602814\n",
      "1  batchidx:  699  loss:  1.2340573370456696\n",
      "1  batchidx:  709  loss:  0.918386971950531\n",
      "1  batchidx:  719  loss:  0.9842210799455643\n",
      "1  batchidx:  729  loss:  1.1521175384521485\n",
      "1  batchidx:  739  loss:  1.2009329080581665\n",
      "1  batchidx:  749  loss:  1.1619853317737578\n",
      "1  batchidx:  759  loss:  0.9355120062828064\n",
      "1  batchidx:  769  loss:  1.0811660945415498\n",
      "1  batchidx:  779  loss:  1.070440974831581\n",
      "1  batchidx:  789  loss:  1.0229361742734908\n",
      "1  batchidx:  799  loss:  1.1441600143909454\n",
      "1  batchidx:  809  loss:  0.8436349719762802\n",
      "1  batchidx:  819  loss:  1.0741843163967133\n",
      "1  batchidx:  829  loss:  0.8561601638793945\n",
      "1  batchidx:  839  loss:  1.0147211492061614\n",
      "1  batchidx:  849  loss:  1.0012377232313157\n",
      "1  batchidx:  859  loss:  1.0409867703914641\n",
      "1  batchidx:  869  loss:  1.2799649596214295\n",
      "1  batchidx:  879  loss:  1.1022928267717362\n",
      "1  batchidx:  889  loss:  0.8909685432910919\n",
      "1  batchidx:  899  loss:  1.2506908535957337\n",
      "1  batchidx:  909  loss:  1.1682914972305298\n",
      "1  batchidx:  919  loss:  1.1582359671592712\n",
      "1  batchidx:  929  loss:  1.02701845318079\n",
      "1  batchidx:  939  loss:  0.9542080104351044\n",
      "1  batchidx:  949  loss:  1.0674274563789368\n",
      "1  batchidx:  959  loss:  1.1168242752552033\n",
      "1  batchidx:  969  loss:  1.02421133518219\n",
      "1  batchidx:  979  loss:  1.0646466374397279\n",
      "1  batchidx:  989  loss:  1.2435933113098145\n",
      "1  batchidx:  999  loss:  0.9540379285812378\n",
      "1  batchidx:  1009  loss:  1.018968152999878\n",
      "1  batchidx:  1019  loss:  1.198278194665909\n",
      "1  batchidx:  1029  loss:  0.9426090002059937\n",
      "1  batchidx:  1039  loss:  1.2031609535217285\n",
      "1  batchidx:  1049  loss:  1.1782790005207062\n",
      "1  batchidx:  1059  loss:  1.3524004518985748\n",
      "1  batchidx:  1069  loss:  1.3662858426570892\n",
      "1  batchidx:  1079  loss:  1.1006893813610077\n",
      "1  batchidx:  1089  loss:  1.018387532234192\n",
      "1  batchidx:  1099  loss:  1.0449157178401947\n",
      "1  batchidx:  1109  loss:  1.1183764576911925\n",
      "1  batchidx:  1119  loss:  1.2020499408245087\n",
      "1  batchidx:  1129  loss:  0.9119600653648376\n",
      "1  batchidx:  1139  loss:  0.9735110998153687\n",
      "1  batchidx:  1149  loss:  1.308714684844017\n",
      "1  batchidx:  1159  loss:  1.1419076442718505\n",
      "1  batchidx:  1169  loss:  0.9752443313598633\n",
      "1  batchidx:  1179  loss:  1.358728152513504\n",
      "1  batchidx:  1189  loss:  0.9256690621376038\n",
      "1  batchidx:  1199  loss:  1.407489037513733\n",
      "1  batchidx:  1209  loss:  1.1052784383296967\n",
      "1  batchidx:  1219  loss:  1.1746153116226197\n",
      "1  batchidx:  1229  loss:  1.0256497025489808\n",
      "1  batchidx:  1239  loss:  1.1627990484237671\n",
      "1  batchidx:  1249  loss:  1.1041026890277863\n",
      "1  batchidx:  1259  loss:  1.32388796210289\n",
      "1  batchidx:  1269  loss:  0.9870584666728973\n",
      "1  batchidx:  1279  loss:  1.2307780265808106\n",
      "1  batchidx:  1289  loss:  1.0948546648025512\n",
      "1  batchidx:  1299  loss:  0.9677029967308044\n",
      "1  batchidx:  1309  loss:  0.9960154175758362\n",
      "1  batchidx:  1319  loss:  1.1325978457927703\n",
      "1  batchidx:  1329  loss:  1.2691523373126983\n",
      "1  batchidx:  1339  loss:  1.2150295972824097\n",
      "1  batchidx:  1349  loss:  1.132790094614029\n",
      "1  batchidx:  1359  loss:  1.0053803771734238\n",
      "1  batchidx:  1369  loss:  0.9510072588920593\n",
      "1  batchidx:  1379  loss:  1.127462500333786\n",
      "1  batchidx:  1389  loss:  1.2377381861209868\n",
      "1  batchidx:  1399  loss:  1.4475714147090912\n",
      "1  batchidx:  1409  loss:  1.0497123897075653\n",
      "1  batchidx:  1419  loss:  0.9835880339145661\n",
      "1  batchidx:  1429  loss:  1.097035413980484\n",
      "1  batchidx:  1439  loss:  1.321685439348221\n",
      "1  batchidx:  1449  loss:  1.2267341792583466\n",
      "1  batchidx:  1459  loss:  1.2227514147758485\n",
      "1  batchidx:  1469  loss:  1.0369717359542847\n",
      "1  batchidx:  1479  loss:  1.0723773837089539\n",
      "1  batchidx:  1489  loss:  1.1683552473783494\n",
      "1  batchidx:  1499  loss:  0.9630939304828644\n",
      "1  batchidx:  1509  loss:  1.1846343100070953\n",
      "1  batchidx:  1519  loss:  1.177920937538147\n",
      "1  batchidx:  1529  loss:  1.0693552255630494\n",
      "1  batchidx:  1539  loss:  1.1609881520271301\n",
      "1  batchidx:  1549  loss:  1.1519453078508377\n",
      "1  batchidx:  1559  loss:  1.2742006838321687\n",
      "1  batchidx:  1569  loss:  1.192237663269043\n",
      "1  batchidx:  1579  loss:  0.9044245988130569\n",
      "1  batchidx:  1589  loss:  1.0327090442180633\n",
      "1  batchidx:  1599  loss:  1.3460121273994445\n",
      "1  batchidx:  1609  loss:  0.9687233448028565\n",
      "1  batchidx:  1619  loss:  1.233488494157791\n",
      "1  batchidx:  1629  loss:  1.1655163645744324\n",
      "1  batchidx:  1639  loss:  1.2379213452339173\n",
      "1  batchidx:  1649  loss:  1.1388429701328278\n",
      "1  batchidx:  1659  loss:  1.077367264032364\n",
      "1  batchidx:  1669  loss:  1.1265862047672273\n",
      "1  batchidx:  1679  loss:  1.0492069184780122\n",
      "1  batchidx:  1689  loss:  1.0576804161071778\n",
      "1  batchidx:  1699  loss:  1.368235421180725\n",
      "1  batchidx:  1709  loss:  1.1833281815052032\n",
      "1  batchidx:  1719  loss:  0.9454547226428985\n",
      "1  batchidx:  1729  loss:  1.0918999552726745\n",
      "1  batchidx:  1739  loss:  1.1179381251335143\n",
      "1  batchidx:  1749  loss:  1.3095112919807435\n",
      "1  batchidx:  1759  loss:  1.177860164642334\n",
      "1  batchidx:  1769  loss:  1.1017549514770508\n",
      "1  batchidx:  1779  loss:  1.135897809267044\n",
      "1  batchidx:  1789  loss:  0.991599977016449\n",
      "1  batchidx:  1799  loss:  1.2924080729484557\n",
      "1  batchidx:  1809  loss:  1.0656362056732178\n",
      "1  batchidx:  1819  loss:  0.9956621050834655\n",
      "1  batchidx:  1829  loss:  1.0819175779819488\n",
      "1  batchidx:  1839  loss:  1.0023265838623048\n",
      "1  batchidx:  1849  loss:  1.0642863839864731\n",
      "1  batchidx:  1859  loss:  1.1674961149692535\n",
      "1  batchidx:  1869  loss:  1.0443451464176179\n",
      "1  batchidx:  1879  loss:  1.1748016238212586\n",
      "1  batchidx:  1889  loss:  1.0279159009456635\n",
      "1  batchidx:  1899  loss:  1.1359159886837005\n",
      "1  batchidx:  1909  loss:  1.1943978428840638\n",
      "1  batchidx:  1919  loss:  1.206553202867508\n",
      "1  batchidx:  1929  loss:  1.1286026209592819\n",
      "1  batchidx:  1939  loss:  1.2654996514320374\n",
      "1  batchidx:  1949  loss:  0.8894957065582275\n",
      "1  batchidx:  1959  loss:  1.2026547729969024\n",
      "1  batchidx:  1969  loss:  0.9681435406208039\n",
      "1  batchidx:  1979  loss:  0.981771981716156\n",
      "1  batchidx:  1989  loss:  1.109551227092743\n",
      "1  batchidx:  1999  loss:  0.7973352909088135\n",
      "1  batchidx:  2009  loss:  1.102680605649948\n",
      "1  batchidx:  2019  loss:  1.260859876871109\n",
      "1  batchidx:  2029  loss:  1.1496732205152511\n",
      "1 loss: 1.0865293140351684\n",
      "2  batchidx:  9  loss:  0.4827718064188957\n",
      "2  batchidx:  19  loss:  0.4703444167971611\n",
      "2  batchidx:  29  loss:  0.3971762046217918\n",
      "2  batchidx:  39  loss:  0.2908218041062355\n",
      "2  batchidx:  49  loss:  0.4714950770139694\n",
      "2  batchidx:  59  loss:  0.46967785656452177\n",
      "2  batchidx:  69  loss:  0.3689733013510704\n",
      "2  batchidx:  79  loss:  0.4863650232553482\n",
      "2  batchidx:  89  loss:  0.2741352140903473\n",
      "2  batchidx:  99  loss:  0.3977845773100853\n",
      "2  batchidx:  109  loss:  0.47230918407440187\n",
      "2  batchidx:  119  loss:  0.3928405150771141\n",
      "2  batchidx:  129  loss:  0.3131850138306618\n",
      "2  batchidx:  139  loss:  0.3730591833591461\n",
      "2  batchidx:  149  loss:  0.35088691487908363\n",
      "2  batchidx:  159  loss:  0.4809178881347179\n",
      "2  batchidx:  169  loss:  0.48310243189334867\n",
      "2  batchidx:  179  loss:  0.44738956689834597\n",
      "2  batchidx:  189  loss:  0.39597336649894715\n",
      "2  batchidx:  199  loss:  0.4120743900537491\n",
      "2  batchidx:  209  loss:  0.31894020885229113\n",
      "2  batchidx:  219  loss:  0.41992047876119615\n",
      "2  batchidx:  229  loss:  0.3731690250337124\n",
      "2  batchidx:  239  loss:  0.375524327903986\n",
      "2  batchidx:  249  loss:  0.43174282312393186\n",
      "2  batchidx:  259  loss:  0.4109208658337593\n",
      "2  batchidx:  269  loss:  0.5805758886039257\n",
      "2  batchidx:  279  loss:  0.41099852323532104\n",
      "2  batchidx:  289  loss:  0.42257285714149473\n",
      "2  batchidx:  299  loss:  0.3617258742451668\n",
      "2  batchidx:  309  loss:  0.4076895348727703\n",
      "2  batchidx:  319  loss:  0.46135255694389343\n",
      "2  batchidx:  329  loss:  0.5890396371483803\n",
      "2  batchidx:  339  loss:  0.41938475966453553\n",
      "2  batchidx:  349  loss:  0.4454274222254753\n",
      "2  batchidx:  359  loss:  0.32759478986263274\n",
      "2  batchidx:  369  loss:  0.42296968400478363\n",
      "2  batchidx:  379  loss:  0.3398118495941162\n",
      "2  batchidx:  389  loss:  0.3907398790121078\n",
      "2  batchidx:  399  loss:  0.45364160239696505\n",
      "2  batchidx:  409  loss:  0.3838806964457035\n",
      "2  batchidx:  419  loss:  0.453681081533432\n",
      "2  batchidx:  429  loss:  0.42968964725732806\n",
      "2  batchidx:  439  loss:  0.5900878086686134\n",
      "2  batchidx:  449  loss:  0.5212301194667817\n",
      "2  batchidx:  459  loss:  0.47588830441236496\n",
      "2  batchidx:  469  loss:  0.459647162258625\n",
      "2  batchidx:  479  loss:  0.442865639179945\n",
      "2  batchidx:  489  loss:  0.4657603681087494\n",
      "2  batchidx:  499  loss:  0.4119038820266724\n",
      "2  batchidx:  509  loss:  0.32195640802383424\n",
      "2  batchidx:  519  loss:  0.33121723905205724\n",
      "2  batchidx:  529  loss:  0.5843372166156768\n",
      "2  batchidx:  539  loss:  0.46957077980041506\n",
      "2  batchidx:  549  loss:  0.5493576884269714\n",
      "2  batchidx:  559  loss:  0.42206763923168183\n",
      "2  batchidx:  569  loss:  0.4338837772607803\n",
      "2  batchidx:  579  loss:  0.3461144037544727\n",
      "2  batchidx:  589  loss:  0.3938560489565134\n",
      "2  batchidx:  599  loss:  0.39101348966360094\n",
      "2  batchidx:  609  loss:  0.5257992327213288\n",
      "2  batchidx:  619  loss:  0.6304652392864227\n",
      "2  batchidx:  629  loss:  0.47149517834186555\n",
      "2  batchidx:  639  loss:  0.40221219807863234\n",
      "2  batchidx:  649  loss:  0.6170779556035996\n",
      "2  batchidx:  659  loss:  0.47475226372480395\n",
      "2  batchidx:  669  loss:  0.4092465117573738\n",
      "2  batchidx:  679  loss:  0.6381884545087815\n",
      "2  batchidx:  689  loss:  0.5433194793760776\n",
      "2  batchidx:  699  loss:  0.41673975586891177\n",
      "2  batchidx:  709  loss:  0.4257752984762192\n",
      "2  batchidx:  719  loss:  0.6391203671693801\n",
      "2  batchidx:  729  loss:  0.5844718918204308\n",
      "2  batchidx:  739  loss:  0.4639964684844017\n",
      "2  batchidx:  749  loss:  0.5684260159730912\n",
      "2  batchidx:  759  loss:  0.43222217857837675\n",
      "2  batchidx:  769  loss:  0.44163173586130144\n",
      "2  batchidx:  779  loss:  0.42892265021800996\n",
      "2  batchidx:  789  loss:  0.3808669582009315\n",
      "2  batchidx:  799  loss:  0.32901680171489717\n",
      "2  batchidx:  809  loss:  0.3388772241771221\n",
      "2  batchidx:  819  loss:  0.47763813436031344\n",
      "2  batchidx:  829  loss:  0.6512277729809284\n",
      "2  batchidx:  839  loss:  0.48706082403659823\n",
      "2  batchidx:  849  loss:  0.33009482473134993\n",
      "2  batchidx:  859  loss:  0.5136923909187316\n",
      "2  batchidx:  869  loss:  0.43419823944568636\n",
      "2  batchidx:  879  loss:  0.537955567240715\n",
      "2  batchidx:  889  loss:  0.7394995719194413\n",
      "2  batchidx:  899  loss:  0.4445499494671822\n",
      "2  batchidx:  909  loss:  0.48488630205392835\n",
      "2  batchidx:  919  loss:  0.44335468113422394\n",
      "2  batchidx:  929  loss:  0.4481485217809677\n",
      "2  batchidx:  939  loss:  0.6544991850852966\n",
      "2  batchidx:  949  loss:  0.5140346258878707\n",
      "2  batchidx:  959  loss:  0.874970543384552\n",
      "2  batchidx:  969  loss:  0.5147268235683441\n",
      "2  batchidx:  979  loss:  0.5877803772687912\n",
      "2  batchidx:  989  loss:  0.5469080209732056\n",
      "2  batchidx:  999  loss:  0.5592542365193367\n",
      "2  batchidx:  1009  loss:  0.536934831738472\n",
      "2  batchidx:  1019  loss:  0.4386390447616577\n",
      "2  batchidx:  1029  loss:  0.5447811245918274\n",
      "2  batchidx:  1039  loss:  0.43670240640640257\n",
      "2  batchidx:  1049  loss:  0.2994075521826744\n",
      "2  batchidx:  1059  loss:  0.38294968456029893\n",
      "2  batchidx:  1069  loss:  0.6311337530612946\n",
      "2  batchidx:  1079  loss:  0.60439832508564\n",
      "2  batchidx:  1089  loss:  0.4767957299947739\n",
      "2  batchidx:  1099  loss:  0.4869336947798729\n",
      "2  batchidx:  1109  loss:  0.4368656799197197\n",
      "2  batchidx:  1119  loss:  0.592812129855156\n",
      "2  batchidx:  1129  loss:  0.661839235946536\n",
      "2  batchidx:  1139  loss:  0.3405425660312176\n",
      "2  batchidx:  1149  loss:  0.6539905190467834\n",
      "2  batchidx:  1159  loss:  0.47799565494060514\n",
      "2  batchidx:  1169  loss:  0.5055241316556931\n",
      "2  batchidx:  1179  loss:  0.5632781535387039\n",
      "2  batchidx:  1189  loss:  0.46390914022922514\n",
      "2  batchidx:  1199  loss:  0.39095178842544553\n",
      "2  batchidx:  1209  loss:  0.5497667849063873\n",
      "2  batchidx:  1219  loss:  0.7029906705021858\n",
      "2  batchidx:  1229  loss:  0.4211121827363968\n",
      "2  batchidx:  1239  loss:  0.6737071067094803\n",
      "2  batchidx:  1249  loss:  0.4392711013555527\n",
      "2  batchidx:  1259  loss:  0.48145916163921354\n",
      "2  batchidx:  1269  loss:  0.5024301588535309\n",
      "2  batchidx:  1279  loss:  0.4995803773403168\n",
      "2  batchidx:  1289  loss:  0.6861655592918396\n",
      "2  batchidx:  1299  loss:  0.596391174197197\n",
      "2  batchidx:  1309  loss:  0.5424488306045532\n",
      "2  batchidx:  1319  loss:  0.49139014482498167\n",
      "2  batchidx:  1329  loss:  0.4541388973593712\n",
      "2  batchidx:  1339  loss:  0.5088298842310905\n",
      "2  batchidx:  1349  loss:  0.5176710039377213\n",
      "2  batchidx:  1359  loss:  0.41336738914251325\n",
      "2  batchidx:  1369  loss:  0.5449441790580749\n",
      "2  batchidx:  1379  loss:  0.6077848501503468\n",
      "2  batchidx:  1389  loss:  0.5489590287208557\n",
      "2  batchidx:  1399  loss:  0.5852827861905098\n",
      "2  batchidx:  1409  loss:  0.46820864304900167\n",
      "2  batchidx:  1419  loss:  0.6393907055258751\n",
      "2  batchidx:  1429  loss:  0.5903458833694458\n",
      "2  batchidx:  1439  loss:  0.5137516260147095\n",
      "2  batchidx:  1449  loss:  0.39488802552223207\n",
      "2  batchidx:  1459  loss:  0.5537486717104911\n",
      "2  batchidx:  1469  loss:  0.6670256078243255\n",
      "2  batchidx:  1479  loss:  0.716680371761322\n",
      "2  batchidx:  1489  loss:  0.49784827679395677\n",
      "2  batchidx:  1499  loss:  0.5277735158801079\n",
      "2  batchidx:  1509  loss:  0.5172732502222062\n",
      "2  batchidx:  1519  loss:  0.49594064652919767\n",
      "2  batchidx:  1529  loss:  0.6358934044837952\n",
      "2  batchidx:  1539  loss:  0.50759487003088\n",
      "2  batchidx:  1549  loss:  0.48569787442684176\n",
      "2  batchidx:  1559  loss:  0.47096350193023684\n",
      "2  batchidx:  1569  loss:  0.5650754883885384\n",
      "2  batchidx:  1579  loss:  0.6013752728700638\n",
      "2  batchidx:  1589  loss:  0.5113899379968643\n",
      "2  batchidx:  1599  loss:  0.5449420869350433\n",
      "2  batchidx:  1609  loss:  0.3810755729675293\n",
      "2  batchidx:  1619  loss:  0.47964569851756095\n",
      "2  batchidx:  1629  loss:  0.5359208181500434\n",
      "2  batchidx:  1639  loss:  0.440469454228878\n",
      "2  batchidx:  1649  loss:  0.6064362496137619\n",
      "2  batchidx:  1659  loss:  0.5078259065747261\n",
      "2  batchidx:  1669  loss:  0.5361635133624076\n",
      "2  batchidx:  1679  loss:  0.500942862033844\n",
      "2  batchidx:  1689  loss:  0.787151925265789\n",
      "2  batchidx:  1699  loss:  0.5019846230745315\n",
      "2  batchidx:  1709  loss:  0.5094341278076172\n",
      "2  batchidx:  1719  loss:  0.5174340084195137\n",
      "2  batchidx:  1729  loss:  0.4922685459256172\n",
      "2  batchidx:  1739  loss:  0.4924737885594368\n",
      "2  batchidx:  1749  loss:  0.5038594886660576\n",
      "2  batchidx:  1759  loss:  0.6626569576561451\n",
      "2  batchidx:  1769  loss:  0.4231868252158165\n",
      "2  batchidx:  1779  loss:  0.47014264911413195\n",
      "2  batchidx:  1789  loss:  0.6081392437219619\n",
      "2  batchidx:  1799  loss:  0.6282533079385757\n",
      "2  batchidx:  1809  loss:  0.5487051486968995\n",
      "2  batchidx:  1819  loss:  0.47774891182780266\n",
      "2  batchidx:  1829  loss:  0.7144858866930008\n",
      "2  batchidx:  1839  loss:  0.3265722468495369\n",
      "2  batchidx:  1849  loss:  0.7395657315850258\n",
      "2  batchidx:  1859  loss:  0.724122303724289\n",
      "2  batchidx:  1869  loss:  0.7555763214826584\n",
      "2  batchidx:  1879  loss:  0.5379874527454376\n",
      "2  batchidx:  1889  loss:  0.532803726196289\n",
      "2  batchidx:  1899  loss:  0.43958448618650436\n",
      "2  batchidx:  1909  loss:  0.31709480136632917\n",
      "2  batchidx:  1919  loss:  0.6319560036063194\n",
      "2  batchidx:  1929  loss:  0.48483802378177643\n",
      "2  batchidx:  1939  loss:  0.5584945946931839\n",
      "2  batchidx:  1949  loss:  0.5437961429357528\n",
      "2  batchidx:  1959  loss:  0.6577257752418518\n",
      "2  batchidx:  1969  loss:  0.711068069934845\n",
      "2  batchidx:  1979  loss:  0.7622144043445587\n",
      "2  batchidx:  1989  loss:  0.5648127526044846\n",
      "2  batchidx:  1999  loss:  0.6176693201065063\n",
      "2  batchidx:  2009  loss:  0.6626818686723709\n",
      "2  batchidx:  2019  loss:  0.641209277510643\n",
      "2  batchidx:  2029  loss:  0.6231507509946823\n",
      "2 loss: 0.5010617122083361\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "60"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "epoch_loss = net.train(batch_size=16,epochs=3)\n",
    "f = open(\"loss.txt\", \"w\")\n",
    "f.write(str(epoch_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f9ec2a11-a4cc-4cba-89fd-70f22ebe0965",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'mangaone mobile application'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question = \"\"\"What kind of device can access the Twilight Princess manga?\"\"\"\n",
    "context = \"\"\"A Japan-exclusive manga series based on Twilight Princess, penned and illustrated by Akira Himekawa, was first released on February 8, 2016. The series is available solely via publisher Shogakukan's MangaOne mobile application. While the manga adaptation began almost ten years after the initial release of the game on which it is based, it launched only a month before the release of the high-definition remake.\"\"\"\n",
    "net.prediction(question,context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fc9b2391-b968-47b9-aaf8-341de851fc42",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save({'state_dict': net.state_dict(), 'epoch': 3},'bertCluster0-'+str(3) + '.pkl')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
