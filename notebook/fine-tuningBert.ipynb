{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7870213d-452e-4761-bf41-353d3e95c981",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers\n",
    "!pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0efd1339-aca4-4eef-839c-3f8c7dbb7049",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.cuda.empty_cache()\n",
    "torch.cuda.memory_summary(device=None, abbreviated=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "309ef23a-18bb-43f4-84dc-a920963d524b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import csv\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import TensorDataset\n",
    "from PIL import Image, ImageFile\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision import models\n",
    "import torch.utils.model_zoo as model_zoo\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import os\n",
    "import json\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import operator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5df42a46-aeb2-4992-847d-cea7b6c96f59",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda')\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "pre_trained_model = BertModel.from_pretrained(\"bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e6b78e08-8424-40fc-8338-de7618b480fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContextQuType:\n",
    "    def __init__(self, tokenizer, length, ids, context, question):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.codeLength = length\n",
    "        self.ids = ids\n",
    "        self.context = context\n",
    "        self.question = question\n",
    "        self.after_encode = 0\n",
    "        \n",
    "    def convert(self):\n",
    "        if self.after_encode==0:\n",
    "            after_encode = tokenizer(self.question,self.context,max_length=self.codeLength,padding=\"max_length\")\n",
    "        return torch.stack([torch.tensor(after_encode['input_ids'][:512]),torch.tensor(after_encode['token_type_ids'][:512]),torch.tensor(after_encode['attention_mask'][:512])])\n",
    "            \n",
    "class LabelType:\n",
    "    def __init__(self, tokenizer, length, CQu, has_ans, text, start):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.codeLength = length\n",
    "        self.has_ans = has_ans\n",
    "        self.text = text.lower()\n",
    "        self.start = start\n",
    "        self.CQu = CQu\n",
    "    \n",
    "    def convert(self):\n",
    "        # make groundtruth\n",
    "        S = [0.0] * self.codeLength\n",
    "        E = [0.0] * self.codeLength\n",
    "        background = tokenizer.encode(self.CQu.question,self.CQu.context,max_length=self.codeLength,padding=\"max_length\")\n",
    "        if self.has_ans:\n",
    "            after_encode = tokenizer.encode(self.text)[1:-1]\n",
    "            ans_length = len(after_encode)\n",
    "            before_answer = tokenizer.encode(self.CQu.question,self.CQu.context[:self.start])\n",
    "            start_id = len(before_answer) - 1\n",
    "            end_id = start_id + ans_length - 1\n",
    "            if tokenizer.decode(background[start_id:end_id+1])!=self.text:\n",
    "                    lower = max(start_id-3,0)\n",
    "                    upper = min(end_id+4,len(background))\n",
    "                    ans_found = False\n",
    "                    for i in range(lower,upper):\n",
    "                        for j in range(i+1,upper):\n",
    "                            candidate = tokenizer.decode(background[i:j])\n",
    "                            #if len(candidate)>0:\n",
    "                            #    if candidate[0]=='$' and self.text[0]!='$':\n",
    "                            #        candidate=candidate[1:]\n",
    "                            #    if candidate[0]!='$' and self.text[0]=='$':\n",
    "                            #        candidate='$'+candidate\n",
    "                            if candidate==self.text:\n",
    "                                start_id = i\n",
    "                                end_id = j-1\n",
    "                                ans_found = True\n",
    "                                break\n",
    "                            if ans_found:\n",
    "                                break\n",
    "            if start_id >= self.codeLength:\n",
    "                start_id = 0\n",
    "                end_id = 0\n",
    "            end_id = min(end_id,self.codeLength - 1)\n",
    "            S[start_id] = 1.0\n",
    "            E[end_id] = 1.0\n",
    "        else:\n",
    "            S[0] = 1.0\n",
    "            E[0] = 1.0\n",
    "        return torch.stack([torch.tensor(S),torch.tensor(E)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5b96639f-9c3d-4c7b-9b74-fde3927766f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(load_path,tokenizer,length):\n",
    "    with open(load_path) as json_data:\n",
    "        dev = json.load(json_data)['data']\n",
    "    CQu = []\n",
    "    Label = []\n",
    "    for data in dev:\n",
    "        for paragraphs in data['paragraphs']:\n",
    "            context = paragraphs['context']\n",
    "            for qas in paragraphs['qas']:\n",
    "                question = qas['question']\n",
    "                ids = qas['id']\n",
    "                ctx = ContextQuType(tokenizer,length,ids,context,question)\n",
    "                CQu.append(ctx)\n",
    "                if qas['is_impossible']:\n",
    "                    Label.append(LabelType(tokenizer,length,ctx,False,'',0))\n",
    "                else:\n",
    "                    answer = qas['answers'][0]\n",
    "                    Label.append(LabelType(tokenizer,length,ctx,True,answer['text'],answer['answer_start']))\n",
    "    return CQu,Label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7dc85e75-9fa2-4082-bd42-fef5da59b14c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OutputPair:\n",
    "    def __init__(self,logits_start,logits_end):\n",
    "        self.logits_start = logits_start\n",
    "        self.logits_end = logits_end\n",
    "\n",
    "class CrossEntropy(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CrossEntropy, self).__init__()\n",
    "    def forward(self,outputs,labels):\n",
    "        log_soft = F.log_softmax(outputs.logits_start,dim=1)\n",
    "        loss_start = F.cross_entropy(log_soft, labels[:,0])\n",
    "        loss_end   = F.cross_entropy(F.log_softmax(outputs.logits_end,dim=1), labels[:,1])\n",
    "        return loss_start + loss_end / 2.0\n",
    "\n",
    "class BERTQuA(nn.Module):\n",
    "    def __init__(self, bert, tokenizer,code_length,device,epsilon = 1.0):\n",
    "        super(BERTQuA, self).__init__()\n",
    "        self.tokenizer = tokenizer\n",
    "        self.device = device\n",
    "        self.encoder = bert.to(device)\n",
    "        self.code_length = code_length\n",
    "        self.output_start = nn.Linear(768,1).to(device)\n",
    "        self.output_end = nn.Linear(768,1).to(device)\n",
    "        self.epsilon = 1.0\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        tokens_X, segments_X, masks = inputs[:,0],inputs[:,1],inputs[:,2]\n",
    "        encoded_X = self.encoder(tokens_X, token_type_ids=segments_X, attention_mask=masks).last_hidden_state\n",
    "        return OutputPair(self.output_start(encoded_X),self.output_end(encoded_X))\n",
    "        \n",
    "    def prediction(self,question,answer_text):\n",
    "        ctx = ContextQuType(self.tokenizer,self.code_length,'',answer_text,question)\n",
    "        inputs = torch.stack([ctx.convert()]).to(device)\n",
    "        token_ids = tokenizer(question,answer_text,max_length=self.code_length,padding=\"max_length\")['input_ids']\n",
    "        outputs = self.forward(inputs)\n",
    "        score_len = outputs.logits_start.shape[1]\n",
    "        start_scores = outputs.logits_start.reshape(score_len)\n",
    "        end_scores = outputs.logits_end.reshape(score_len)\n",
    "        no_ans_scores = start_scores[0] + end_scores[0]\n",
    "        answer = ''\n",
    "        flag = False\n",
    "        tokens = tokenizer.convert_ids_to_tokens(token_ids)\n",
    "        if (torch.max(start_scores) + torch.max(end_scores) <= no_ans_scores +  self.epsilon):\n",
    "            answer = ''\n",
    "        else:\n",
    "            answer_start = torch.argmax(start_scores)\n",
    "            answer_end = torch.argmax(end_scores)\n",
    "            answer = self.tokenizer.decode(token_ids[answer_start:answer_end+1])\n",
    "        return answer\n",
    "    \n",
    "    def predictionLabel(self,label):\n",
    "        #ctx = ContextQuType(self.tokenizer,self.code_length,'',answer_text,question)\n",
    "        #inputs = torch.stack([ctx.convert()]).to(device)\n",
    "        #token_ids = tokenizer(question,answer_text,max_length=self.code_length,padding=\"max_length\")['input_ids']\n",
    "        #outputs = self.forward(inputs)\n",
    "        outputs = label.convert()\n",
    "        #print(outputs)\n",
    "        token_ids = tokenizer(label.CQu.question,label.CQu.context,max_length=self.code_length,padding=\"max_length\")['input_ids']\n",
    "        start_scores = outputs[0,:]\n",
    "        end_scores = outputs[1,:]\n",
    "        #print(start_scores,end_scores)\n",
    "        no_ans_scores = start_scores[0] + end_scores[0]\n",
    "        answer = ''\n",
    "        tokens = tokenizer.convert_ids_to_tokens(token_ids)\n",
    "        #print(outputs.logits_start.reshape(-1))\n",
    "        #print(outputs.logits_end.reshape(-1))\n",
    "        if (torch.max(start_scores) + torch.max(end_scores) <= no_ans_scores +  self.epsilon):\n",
    "            answer = ''\n",
    "        else:\n",
    "            answer_start = torch.argmax(start_scores)\n",
    "            answer_end = torch.argmax(end_scores)\n",
    "            answer = self.tokenizer.decode(token_ids[answer_start:answer_end+1])\n",
    "        return answer\n",
    "    def load_data(self,file_path):\n",
    "        CQu, La = load_data(file_path,self.tokenizer,self.code_length)\n",
    "        self.inputs = torch.stack([ctx.convert() for ctx in CQu])\n",
    "        self.labels = torch.stack([ctx.convert() for ctx in La])\n",
    "    \n",
    "    def train(self, epochs=3, batch_size = 48, lr = 3e-5, T=10):\n",
    "        optimizer = optim.Adam(self.parameters(), lr=lr)\n",
    "        criterion = CrossEntropy().to(device)\n",
    "        self.train_loader = DataLoader(TensorDataset(self.inputs,self.labels),batch_size = batch_size, shuffle = True, num_workers = 2)\n",
    "        epoch_loss = []\n",
    "        for epoch in range(epochs):      \n",
    "            running_loss = 0.0\n",
    "            epoch_running_loss = 0.0\n",
    "            batch_count = 0\n",
    "            for batchidx, (x, label) in enumerate(self.train_loader):\n",
    "                x, label = x.to(device), label.to(device)\n",
    "                output = self.forward(x)\n",
    "                output.logits_start=output.logits_start.resize(len(output.logits_start),self.code_length)\n",
    "                output.logits_end=output.logits_end.resize(len(output.logits_end),self.code_length)\n",
    "                loss = criterion(output, label)\n",
    "                # backprop\n",
    "                optimizer.zero_grad()  #梯度清0\n",
    "                loss.backward()   #梯度反传\n",
    "                optimizer.step()   #保留梯度\n",
    "                \n",
    "                running_loss += loss.item()\n",
    "                epoch_running_loss += loss.item()\n",
    "                batch_count += 1\n",
    "                if batchidx % T == T-1:\n",
    "                    print(epoch,' batchidx: ', batchidx, ' loss: ', running_loss/T)\n",
    "                    running_loss = 0.0\n",
    "            epoch_loss.append(epoch_running_loss/batch_count)\n",
    "            print(epoch, 'loss:', epoch_running_loss/batch_count)\n",
    "        return epoch_loss\n",
    "    def printLabel(self,file_path):\n",
    "        CQu, La = load_data(file_path,self.tokenizer,self.code_length)\n",
    "        count = 0\n",
    "        precise = 0\n",
    "        for label in La:\n",
    "            count += 1\n",
    "            predict = self.predictionLabel(label)\n",
    "            if predict==label.text:\n",
    "                precise += 1\n",
    "            else:\n",
    "                print(predict,label.text,label.has_ans)\n",
    "        print(precise/count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "50ec9d62-0604-4eed-83c3-8b6e1aeaa123",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda')\n",
    "net = BERTQuA(pre_trained_model,tokenizer,512,device)\n",
    "net.load_data('train-v2.0-Tag1.0.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48ba42be-008a-4086-a94b-9175f2f4c8b1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "epoch_loss = net.train(batch_size=16,epochs=3)\n",
    "f = open(\"loss.txt\", \"w\")\n",
    "f.write(str(epoch_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f9ec2a11-a4cc-4cba-89fd-70f22ebe0965",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'mangaone mobile application'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question = \"\"\"What kind of device can access the Twilight Princess manga?\"\"\"\n",
    "context = \"\"\"A Japan-exclusive manga series based on Twilight Princess, penned and illustrated by Akira Himekawa, was first released on February 8, 2016. The series is available solely via publisher Shogakukan's MangaOne mobile application. While the manga adaptation began almost ten years after the initial release of the game on which it is based, it launched only a month before the release of the high-definition remake.\"\"\"\n",
    "net.prediction(question,context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fc9b2391-b968-47b9-aaf8-341de851fc42",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save({'state_dict': net.state_dict(), 'epoch': 3},'bertCluster0-'+str(3) + '.pkl')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
